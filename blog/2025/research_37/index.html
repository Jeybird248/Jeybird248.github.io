<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_37/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts</h1> <p class="post-meta"> Created in August 03, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a>   <a href="/blog/tag/neuron-analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> Neuron Analysis</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This paper introduces Query-Relevant Neuron Cluster Attribution (QRNCA), a framework aimed at identifying query-relevant (QR) neurons in large, decoder-only language models, especially in the context of long-form (multi-choice) text generation. The work addresses gaps in the localization of internal model knowledge, empirically validating new datasets to cover both domain and language knowledge. The analysis reveals the existence of localized knowledge regions within LLMs and demonstrates potential downstream applications in knowledge editing and neuron-based prediction.</p> <h2 id="key-insights">Key Insights</h2> <p>QRNCA extends prior attribution and knowledge localization techniques (i.e. Knowledge Attribution, Dai et al. 2022) to modern, decoder-only LLMs like Llama and Mistral using prompt engineering to transform arbitrary queries into constrained multi-choice QA tasks, allowing gradient-based attribution at the neuron level.</p> <p>The framework calculates neuron attribution using adapted gradients for GLU-based FFNs, aggregates neuron clusters over query variants, introduces inverse cluster attribution (ICA) to downweight neurons common across queries, and prunes ‘common neurons’ (those associated with general or high-frequency tokens).</p> <p>QRNCA reliably identifies neurons whose activation modulates specific knowledge predictions, surpassing activation-based and earlier knowledge attribution baselines in their probability change ratio (PCR) metric. Notably, the method reveals domain-specific QR neurons distributed mainly in mid-to-top transformer layers, while language-specific neurons are more diffusely distributed.</p> <p>The heatmap analyses indicate domain knowledge is manifested in relatively concentrated neuron clusters, often in mid-layers, whereas language concepts are more dispersed. This aligns with prior findings on hierarchical abstraction in transformer models.</p> <h2 id="example">Example</h2> <p>Suppose a model is given a biology multi-choice question. QRNCA computes neuron attribution scores for each FFN neuron in response to the query, identifies clusters of highly attributed neurons, and prunes those commonly activated across diverse queries (i.e. common neurons like those signaling the letter “A” or frequent stop words). Boosting the activation of identified QR neurons measurably increases the probability of the correct answer, and suppressing them decreases it. By manipulating only these neurons, researchers can “edit” the model’s factual predictions in a targeted, query-specific manner.</p> <h2 id="ratings">Ratings</h2> <p>Novelty: 2/5<br> While QRNCA systematizes neuron attribution for decoder-only LLMs and introduces inverse cluster attribution, the technical core is an incremental adaptation of known approaches (gradient attribution, common neuron filtering, prompt engineering). The underlying methods are not fundamentally new; the main advancement is practical application to larger models and long-form tasks, plus new dataset construction and analysis.</p> <p>Clarity: 3/5<br> The exposition suffers from ambiguity around the precise algorithmic contributions relative to prior work. The paper has references to previous methodology, which sometimes obscures what is truly novel; greater emphasis on comparative summary and clearer demarcation of new techniques vs. adaptation would be beneficial.</p> <h2 id="personal-comments">Personal Comments</h2> <p>QRNCA is a useful step in the ongoing march from “knowledge neuron” studies in compact models (BERT, GPT-2) towards the realities of modern, much larger and more complex LLMs. The triage of attribution-based neuron selection, inverse cluster attribution, and common neuron pruning is sensible and well-motivated by empirical limitations of existing methods when faced with the combinatorial sprawl of open-domain knowledge in LLMs.</p> <p>Nevertheless, the work is not a paradigm shift. The central innovation of performing neuron attribution in decoder-only, long-form settings, while nontrivial in engineering, is not a conceptual leap beyond existing attribution and mediation frameworks. The study is clearer in its problems tackled than in the novelty of its solutions. There is some missed opportunity in not pushing further into questions of <em>why</em> some concepts (across domains or languages) are more or less localizable, or systematically analyzing which types of knowledge are easiest vs. hardest to pin down neuronally. These directions would make both interpretability and mechanistic understanding better.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_21/">Literature Review: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_20/">Literature Review: Thinkless: LLM Learns When to Think</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_39/">Literature Review: Learning without training: The implicit dynamics of in-context learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_7/">Literature Review: Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_35/">Literature Review: Universal Jailbreak Suffixes Are Strong Attention Hijackers</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>