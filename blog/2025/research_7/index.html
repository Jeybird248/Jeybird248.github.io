<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_7/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey</h1> <p class="post-meta"> Created in May 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/cybersecurity"> <i class="fa-solid fa-hashtag fa-sm"></i> Cybersecurity</a>   <a href="/blog/tag/ai-security"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Security</a>   <a href="/blog/tag/vulnerability-detection"> <i class="fa-solid fa-hashtag fa-sm"></i> Vulnerability Detection</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="summary">Summary</h2> <ul> <li>Provides a systematic survey on the applications of Large Language Models (LLMs) in cybersecurity, organized by the cyber attack lifecycle: reconnaissance, foothold establishment, lateral movement, data exfiltration, and post-exfiltration.</li> <li>Highlights LLMs’ strengths in: <ul> <li>Detecting covert reconnaissance (system-based and human-based, e.g., phishing, malware) using advanced pattern recognition, reasoning, and few-shot learning.</li> <li>Automating vulnerability detection, analysis, and patching in the foothold phase, leveraging code property graphs, fine-tuning, and prompt engineering.</li> <li>Enhancing intrusion detection, anomaly detection, and endpoint response during lateral movement through log, traffic, and endpoint data analysis.</li> </ul> </li> <li>Identifies a research gap: limited LLM application for post-intrusion scenarios (data exfiltration, post-exfiltration) and calls for more research in these areas.</li> <li>Details LLMs’ roles in Cyber Threat Intelligence (CTI): <ul> <li>Automating CTI collection and processing from unstructured sources (forums, reports).</li> <li>Structuring intelligence into knowledge graphs and extracting actionable insights.</li> <li>Providing strategic reasoning and defense recommendations.</li> </ul> </li> <li>Discusses deployment strategies: <ul> <li>Traditional networks: centralized/cloud-based LLM deployment.</li> <li>Next-generation networks (IoT, 6G, SAGIN): resource constraints addressed via model pruning, federated/distributed/split learning, and hybrid architectures.</li> </ul> </li> <li>Reviews security risks: <ul> <li>External: prompt injection and data poisoning attacks, with mitigation via structured input, fine-tuning, and output validation.</li> <li>Inherent: hallucinations/misinformation, mitigated by RAG, factuality-enhanced training, and mediator components.</li> </ul> </li> <li>Outlines open problems and future directions: <ul> <li>Need for high-quality, domain-specific datasets; overcoming input length limitations.</li> <li>Defending against targeted and pollution attacks.</li> <li>Improving real-time inference, interpretability, and deployment in next-gen networks.</li> <li>Expanding LLM application coverage and ensuring transparency/reproducibility.</li> </ul> </li> </ul> <h2 id="key-insights">Key Insights</h2> <p><strong>1. LLMs Across the Cyber Attack Lifecycle</strong></p> <ul> <li> <strong>Reconnaissance Phase</strong>: LLMs excel at detecting both system-based (i.e. network scans, log analysis) and human-based (i.e. phishing, social engineering) reconnaissance through advanced pattern recognition, reasoning, and few-shot learning. LLM-powered honeypots (i.e. shelLM) can convincingly simulate real systems to deceive attackers.</li> <li> <strong>Foothold Establishment</strong>: LLMs are effective in vulnerability detection, analysis, and patching. Techniques like code property graphs, multitask fine-tuning, and prompt engineering allow LLMs to identify and remediate vulnerabilities, generate patches, and validate repairs. However, input length limitations remain a bottleneck for large-scale or cross-file vulnerabilities.</li> <li> <strong>Lateral Movement</strong>: LLMs enhance intrusion detection systems (IDS), anomaly detection, and endpoint detection and response (EDR) by parsing complex logs, network traffic, and endpoint data. Their adaptability allows for improved detection of novel or sophisticated attacks compared to rule-based systems.</li> <li> <strong>Post-Intrusion Gaps</strong>: The survey highlights a significant research gap in LLM applications for post-intrusion scenarios (data exfiltration, post-exfiltration), calling for more work on defense strategies in these critical phases.</li> </ul> <p><strong>2. LLMs in Cyber Threat Intelligence (CTI)</strong></p> <ul> <li>LLMs automate CTI collection, processing, and analysis, extracting actionable intelligence from unstructured sources like cybercrime forums and threat reports.</li> <li>They facilitate the conversion of CTI into structured knowledge graphs, generate organization-specific intelligence, and support strategic reasoning for defense recommendations.</li> <li>LLMs reduce manual effort and improve the timeliness and quality of CTI, but face risks from data source contamination and adversarial manipulation.</li> </ul> <p><strong>3. Deployment in Network Scenarios</strong></p> <ul> <li> <strong>Traditional Networks</strong>: Centralized deployment in cloud or local servers is common, leveraging LLMs for detection, analysis, and policy generation.</li> <li> <strong>Next-Generation Networks</strong>: Resource constraints and heterogeneity in IoT, 6G, and integrated networks present deployment challenges. Solutions include model pruning, federated learning, distributed and split learning, and hybrid microservice architectures. LLMs are poised to become integral to intelligent, adaptive security management in these environments.</li> </ul> <p><strong>4. Security Risks of LLMs</strong></p> <ul> <li> <strong>External Threats</strong>: Prompt injection and data poisoning attacks threaten LLM integrity. Defense strategies include structured input separation, task-specific fine-tuning, and output validation.</li> <li> <strong>Inherent Risks</strong>: Hallucinations and misinformation can lead to incorrect or unsafe outputs, potentially undermining security systems. Techniques like RAG, factuality-enhanced training, and mediator components are proposed to mitigate these risks.</li> </ul> <p><strong>5. Open Problems and Research Directions</strong></p> <ul> <li>Scarcity of high-quality, domain-specific datasets and input length limitations hinder LLM performance in complex cybersecurity tasks.</li> <li>Targeted attacks, slow inference in real-time systems, and over-reliance on black-box models raise concerns about reliability and reproducibility.</li> <li>Future research should focus on open datasets, breaking input length barriers, robust defense against adversarial contamination, semantic data transformation, interpretability, and expanding LLM coverage to next-generation networks.</li> </ul> <h2 id="example">Example</h2> <p><strong>LLM-Driven Phishing Detection:</strong></p> <ul> <li> <em>ChatSpamDetector</em> preprocesses emails for LLM analysis, assigns a spam-detection role via prompt engineering, and uses chain-of-thought prompting to decompose the detection task. This approach enables stepwise, explainable phishing detection, outperforming traditional methods in adaptability and detection of novel attack vectors.</li> </ul> <h2 id="ratings">Ratings</h2> <table> <thead> <tr> <th>Category</th> <th>Score</th> <th>Rationale</th> </tr> </thead> <tbody> <tr> <td>Novelty</td> <td>N/A</td> <td>Survey Paper</td> </tr> <tr> <td>Technical Contribution</td> <td>N/A</td> <td>Survey Paper</td> </tr> <tr> <td>Readability</td> <td>4</td> <td>Well-structured, with clear explanations, comparative tables, and practical examples. Some technical depth may challenge non-experts.</td> </tr> </tbody> </table> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_11/">Literature Review: Large Language Models are Autonomous Cyber Defenders</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_9/">Literature Review: Attack and Defense Techniques in Large Language Models: A Survey and New Perspectives</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_2/">Literature Review: Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_8/">Literature Review: Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_3/">Literature Review: Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>