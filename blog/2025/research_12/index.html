<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_12/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models</h1> <p class="post-meta"> Created in May 19, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/chain-of-thought"> <i class="fa-solid fa-hashtag fa-sm"></i> Chain-of-Thought</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reasoning</a>   <a href="/blog/tag/robustness"> <i class="fa-solid fa-hashtag fa-sm"></i> Robustness</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="summary">Summary</h2> <p>This paper investigates a critical weakness in few-shot Chain-of-Thought (CoT) prompting for large language models (LLMs): specific tokens or segments within CoT demonstrations can inadvertently distract the model and degrade reasoning performance. The authors identify that LLMs sometimes over-focus on local information from demonstrations, leading to errors by “taking things out of context.” They propose <strong>Few-shot Attention Intervention (FAI)</strong>, a lightweight method that dynamically analyzes and adjusts attention patterns to suppress the distracting effect of such tokens. Extensive experiments across math, commonsense, and symbolic reasoning benchmarks show that FAI consistently improves accuracy, notably achieving a 5.91% gain on the AQuA dataset.</p> <p align="center"> <img src="../../../assets/img/literature/12_0.png" width="600"> </p> <p align="center"><em>Figure: Overview of the FAI method. FAI identifies and intervenes on distracting tokens in each layer's attention matrix, improving reasoning robustness without significant computational overhead.</em></p> <h2 id="key-insights">Key Insights</h2> <ul> <li> <p><strong>CoT Fragility and Token Distraction:</strong><br> While CoT prompting improves LLM reasoning, the paper demonstrates that even single tokens or phrases in demonstrations can mislead the model. This distraction manifests as the model copying irrelevant details or being implicitly biased by local demonstration artifacts, resulting in incorrect answers.</p> </li> <li> <p><strong>Mechanism Analysis via Attention Saliency:</strong><br> The authors use attention saliency techniques to trace information flow within LLMs at each output step. They find that tokens with high self-attention and low aggregation from other tokens are most likely to distract the model, as their semantic content is preserved and directly influences output generation.</p> </li> <li> <p><strong>Few-shot Attention Intervention (FAI):</strong><br> FAI identifies tokens with insufficient information aggregation (i.e., high self-attention) in the demonstration and blocks their attention flow to output tokens during generation. This intervention is efficient, requiring only lightweight computation and affecting about 15% of demonstration tokens on average.</p> </li> <li> <p><strong>Empirical Validation:</strong><br> FAI delivers consistent accuracy improvements across multiple LLMs (GPT2-XL, GPT-NEO, Llama-3-8B/70B) and datasets (GSM8K, AQuA, CSQA, Big-Bench-Hard, Last Letter Concatenation). The most significant boost is observed on math reasoning tasks, where distracting tokens are prevalent.</p> </li> <li> <p><strong>Ablation and Robustness Studies:</strong><br> The paper constructs “GSMgood” and “GSMbad” subsets to isolate the effect of distraction. FAI notably improves performance on GSMbad (samples vulnerable to distraction) without harming GSMgood (robust samples). Contrastive experiments show that blocking all demonstration-to-output attention suppresses both distraction and the positive effect of CoT, while FAI selectively suppresses only the negative.</p> </li> <li> <p><strong>Token Analysis:</strong><br> FAI primarily flags mathematical symbols and numbers as distracting tokens, aligning with the observed error patterns in math reasoning tasks.</p> </li> </ul> <h2 id="example">Example</h2> <p><strong>Problem:</strong><br> Mary has 6 jars of sprinkles. Each jar can decorate 8 cupcakes. If each pan holds 12 cupcakes, how many pans should she bake?</p> <p><strong>Distracting Demonstration:</strong><br> A prior demonstration included “Each jar can hold 160 quarters,” which the model incorrectly copied, leading to a wrong answer.</p> <p><strong>With FAI:</strong><br> FAI identifies “160” as a distracting token and blocks its influence, allowing the model to correctly reason through the current problem without incorporating irrelevant details from the demonstration.</p> <p align="center"> <img src="../../../assets/img/literature/12_1.png" width="600"> </p> <p align="center"><em>Figure: Attention saliency visualization. Darker cells indicate higher influence. FAI blocks the flow from distracting tokens (e.g., "160") to the output, preventing erroneous copying.</em></p> <h2 id="ratings">Ratings</h2> <table> <thead> <tr> <th>Category</th> <th>Score</th> <th>Comments</th> </tr> </thead> <tbody> <tr> <td>Novelty</td> <td>4</td> <td>The paper identifies and addresses a subtle, underexplored failure mode in CoT prompting, offering a practical and efficient solution. While attention interventions have been studied, their targeted application to CoT distraction is new and insightful.</td> </tr> <tr> <td>Technical Contribution</td> <td>4</td> <td>Introduces a lightweight, actionable method (FAI) grounded in a detailed analysis of LLM attention dynamics. Not a new framework, but a significant improvement to prompting strategies.</td> </tr> <tr> <td>Readability</td> <td>3</td> <td>The paper is thorough and provides clear figures and tables, but the technical sections (especially on saliency and aggregation coefficients) are dense and could be more accessible to non-experts. The motivation and empirical results are well-explained, but some methodological details are mathematically heavy.</td> </tr> </tbody> </table> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_2/">Literature Review: Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_21/">Literature Review: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_33/">Literature Review: SelfElicit - Your Language Model Secretly Knows Where is the Relevant Evidence</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_35/">Literature Review: Universal Jailbreak Suffixes Are Strong Attention Hijackers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_20/">Literature Review: Thinkless: LLM Learns When to Think</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>