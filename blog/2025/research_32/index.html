<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: A Survey on Latent Reasoning | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_32/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: A Survey on Latent Reasoning</h1> <p class="post-meta"> Created in July 13, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/latent-reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> Latent Reasoning</a>   <a href="/blog/tag/chain-of-thought"> <i class="fa-solid fa-hashtag fa-sm"></i> Chain-of-Thought</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/transformer-architecture"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformer Architecture</a>   <a href="/blog/tag/mechanistic-interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Mechanistic Interpretability</a>   <a href="/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This comprehensive survey systematically examines the emerging paradigm of latent reasoning in Large Language Models, where multi-step inference occurs entirely within continuous hidden states rather than through explicit token generation. The work provides a unifying mathematical framework and taxonomy for understanding how models can perform reasoning without the constraints of natural language.</p> <h2 id="key-insights">Key Insights</h2> <p>The paper quantifies a computational asymmetry: explicit reasoning through discrete tokens provides approximately 15 bits of information per step, while latent space operations leverage 40,960 bits (for 2560-dimensional FP16 hidden states), representing a ~2,700× difference in expressive capacity. This bandwidth gap fundamentally reframes the efficiency-performance trade-off in reasoning systems.</p> <p>The authors establish a clear mathematical distinction between two computational approaches. Vertical recurrence (activation-based methods) creates deeper computational graphs through iterative refinement within layers, while horizontal recurrence (hidden state-based methods) expands temporal capacity through compressed state evolution. This dichotomy provides conceptual clarity to a previously fragmented field.</p> <p>The mechanistic interpretability analysis reveals that different network layers systematically specialize for distinct reasoning operations: shallow layers handle syntactic processing and factual retrieval, intermediate layers contain specialized reasoning circuits with superior representational capabilities, and deep layers perform semantic transformation and decision-making. This supports the notion that standard Transformers already implement implicit latent reasoning pipelines.</p> <p>Models like DeltaNet demonstrate mathematical equivalence between their state update rules and single gradient descent steps on regression objectives, suggesting that temporal evolution of hidden states constitutes a form of online learning that trades time for computational depth.</p> <p>Unlike autoregressive generation’s irreversible decisions, diffusion models enable global planning and bidirectional refinement, potentially unlocking reasoning trajectories with no linguistic equivalent.</p> <h2 id="example">Example</h2> <p>Consider the Coconut method as a concrete implementation of training-induced recurrence. Rather than generating explicit reasoning tokens, Coconut inserts the last-layer hidden state of the previous decoding step as a “continuous thought” vector before the current token. This creates a recurrent loop entirely in latent space: the model can perform breadth-first exploration of reasoning paths while reusing the same Transformer parameters. On logical reasoning tasks like PrOntoQA, this approach achieves parity with explicit Chain-of-Thought while eliminating the computational overhead of intermediate token generation.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: N/A (Survey Paper)</strong></p> <p><strong>Clarity: 4/5</strong></p> <p>The paper successfully organizes a complex, rapidly evolving field into coherent categories with clear mathematical formulations. The progression from preliminary frameworks through specific methods to advanced paradigms follows logical structure, though the density of technical content requires careful reading.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This survey arrives at a critical point as the field is conflicted with fundamental questions about the nature of machine reasoning. The bandwidth argument alone justifies serious attention, if we accept that human cognition isn’t constrained to linguistic thinking, why should we impose such limitations on artificial systems?</p> <p>What interests me most is how this work reveals that latent reasoning isn’t merely an optimization trick, but potentially a more natural computational paradigm for neural networks. The mechanistic interpretability evidence suggesting that standard Transformers already implement implicit reasoning pipelines is particularly compelling. We may have been forcing models to articulate thoughts they’re already thinking more efficiently in silence.</p> <p>However, the field suffers from evaluation fragmentation. The authors correctly identify the lack of standardized benchmarks and consistent training methodologies as major limitations. Most studies compare against non-reasoning baselines rather than each other, making it difficult to assess true progress. This is a classic early-stage field problem that will require community coordination to resolve.</p> <p>The infinite-depth reasoning section particularly excites me because it suggests the idea that models could spend arbitrary time refining solutions through iterative latent refinement which feels like a step toward more human-like contemplation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_21/">Literature Review: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_13/">Literature Review: Enhancing Latent Computation in Transformers with Latent Tokens</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_16/">Literature Review: Programming Refusal with Conditional Activation Steering</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_39/">Literature Review: Learning without training: The implicit dynamics of in-context learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_14/">Literature Review: Group Think - Collaborating at Token Level Granularity</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>