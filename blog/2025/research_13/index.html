<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: Enhancing Latent Computation in Transformers with Latent Tokens | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_13/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: Enhancing Latent Computation in Transformers with Latent Tokens</h1> <p class="post-meta"> Created in May 28, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/parameter-efficient-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> Parameter-Efficient-Tuning</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="summary">Summary</h2> <p>This paper introduces “latent tokens” - non-interpretable dummy tokens that provide additional computation time for Transformer-based LLMs during autoregressive generation. The method inserts these tokens at strategic positions (i.e., before commas, periodically, or at sequence boundaries) to enhance model performance through extended attention computation. The authors propose parameter-efficient fine-tuning of only the latent token embeddings while freezing the pre-trained model, and demonstrate improvements particularly in out-of-distribution scenarios through three synthetic tasks testing self-prompting, information retrieval, and instruction adherence.</p> <h2 id="key-insights">Key Insights</h2> <p>The core insight is that LLMs often suffer from insufficient computation time during next-token prediction, particularly for complex reasoning tasks. The latent tokens act as computational “breathing room” - similar to human speech patterns with pauses and fillers. The authors make several important technical contributions:</p> <p><strong>Positional Encoding Design</strong>: Latent tokens share position IDs with their following verbal tokens, preserving the original sequence structure while enabling additional computation. This is crucial for maintaining compatibility with existing Transformer infrastructure.</p> <p><strong>Function Specialization</strong>: Different latent token groups can serve distinct purposes (i.e., start-of-query tokens for instruction memory, comma-positioned tokens for segmentation). This prevents conflicting learning objectives across positions.</p> <p><strong>Parameter Efficiency</strong>: Only the latent token embeddings (typically &lt;1% of model parameters) require training, making the approach highly practical for deployment.</p> <p>The synthetic task analysis reveals three potential mechanisms: (1) self-prompting for maintaining consistency in long generations, (2) serving as “anchors” for information retrieval from input sequences, and (3) improving instruction adherence through distributed memory across the sequence.</p> <h2 id="example">Example</h2> <p>In the Generation task, the model learns to apply a mathematical operation: <code class="language-plaintext highlighter-rouge">44@47=83,47@83=34,83@34=21,...</code> where <code class="language-plaintext highlighter-rouge">a1a2@b1b2 = |a1+b1|mod9|a2-b2|mod9</code>. With latent tokens inserted before each comma (<code class="language-plaintext highlighter-rouge">Comma_2</code>), the model achieves 23% relative improvement over baselines in out-of-distribution scenarios requiring longer sequences than seen during training. Attention visualization shows each latent token group is heavily attended by the subsequent six verbal tokens, suggesting they serve as computational anchors for generating the next equation.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3/5</strong></p> <p>The core concept of adding non-interpretable tokens for computation is not new (pause tokens, filler tokens exist). However, the unified framework with flexible positioning, proper positional encoding design, and function specialization represents a meaningful incremental advancement over prior work.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work exemplifies the current trend of “buying time” for LLMs through various computational augmentation strategies. While the engineering is competent and the parameter-efficiency angle is practically valuable, I share the skepticism about its somewhat engineered nature. The synthetic tasks, while designed to test specific hypotheses, feel contrived rather than emerging from natural problem domains.</p> <p>The attention visualization in Figure 5 showing periodic patterns is intriguing but lacks the depth of analysis needed to truly understand the underlying mechanisms. The 23-220% improvements in OOD scenarios are impressive numerically, but the tasks are so specialized that generalization to real-world applications remains questionable.</p> <p>What concerns me most is the ad-hoc nature of the insertion strategies. The fact that simple periodic insertion (every k tokens) works reasonably well suggests the method may be capturing something more fundamental about sequence processing, but the authors don’t adequately explore this deeper principle.</p> <p>The comparison with pause tokens and filler tokens is fair, but the paper would benefit from more rigorous analysis of when and why latent tokens help, and more importantly <em>why</em> there’s a difference between the two. The function specialization concept shows promise but needs more systematic investigation across diverse task types.</p> <p>This feels like solid incremental work that will likely see practical adoption due to its parameter efficiency, but it’s unlikely to fundamentally change how we think about transformer computation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_21/">Literature Review: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_14/">Literature Review: Group Think - Collaborating at Token Level Granularity</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_16/">Literature Review: Programming Refusal with Conditional Activation Steering</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_20/">Literature Review: Thinkless: LLM Learns When to Think</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_17/">Literature Review: Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>