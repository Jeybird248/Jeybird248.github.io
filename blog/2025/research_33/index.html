<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Literature Review: SelfElicit - Your Language Model Secretly Knows Where is the Relevant Evidence | Jehyeok Yeon </title> <meta name="author" content="Jehyeok Yeon"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeybird248.github.io/blog/2025/research_33/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jehyeok</span> Yeon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Literature Review: SelfElicit - Your Language Model Secretly Knows Where is the Relevant Evidence</h1> <p class="post-meta"> Created in July 13, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   <a href="/blog/tag/attention-mechanisms"> <i class="fa-solid fa-hashtag fa-sm"></i> Attention Mechanisms</a>   <a href="/blog/tag/rag"> <i class="fa-solid fa-hashtag fa-sm"></i> RAG</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This paper introduces SelfElicit, an inference-time method that leverages attention patterns in deeper layers of language models to automatically highlight relevant evidence in context-based question answering. The authors claim that LLMs possess an inherent ability to identify relevant evidence through their attention mechanisms, even when they produce incorrect answers.</p> <h2 id="key-insights">Key Insights</h2> <p>The central finding revolves around attention pattern analysis across transformer layers. The authors demonstrate that deeper layers (particularly the last 50%) consistently assign higher attention scores to evidence sentences compared to non-evidence content, regardless of whether the model ultimately generates correct responses. This observation holds across multiple model families (Llama, Mistral, Qwen) and datasets.</p> <p>The methodology involves computing sentence-level attention scores by averaging token-level attention within sentences, then using these scores to identify and highlight relevant evidence. The highlighting strategy employs simple text markers (<code class="language-plaintext highlighter-rouge">&lt;start_important&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;end_important&gt;</code>) placed around identified evidence sentences, followed by modified prompts that direct the model’s attention to these markers.</p> <p>Performance improvements are consistent but modest: typically 5-12% gains in exact match and F1 scores across various question-answering datasets. The method demonstrates computational efficiency, requiring only one additional forward pass for evidence identification while maintaining the semantic structure of the original context.</p> <h2 id="example">Example</h2> <p>Consider a multi-hop reasoning question about Walter Giffen’s hometown. While the base model incorrectly responds “Norwood” (focusing on his birthplace), SelfElicit highlights both “Walter Frank Giffen 20 September 1861 in Norwood…” and “Norwood is a suburb of Adelaide…” enabling the model to correctly infer “Adelaide” as the final answer.</p> <p align="center"> <img src="../../../assets/img/literature/33_0.png" width="600"> </p> <p align="center"><em>Figure: SelfElicit workflow showing attention-based evidence identification followed by context highlighting to guide model responses toward relevant information.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3/5</strong></p> <p>While attention-based evidence identification is intuitive, the systematic application across layers and the empirical validation across model families provides useful insights. However, the core concept of using attention for relevance is not particularly novel.</p> <p><strong>Clarity: 4/5</strong> The paper is well-structured with clear methodology and comprehensive experiments. The writing effectively communicates both the approach and findings, though some theoretical depth is lacking.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work raises fascinating questions about what we’re actually observing when we see these attention patterns. The authors treat the finding that “LMs have an inherent ability to identify relevant evidence” as an empirical discovery rather than something they fully understand theoretically. This represents a broader challenge in LLM interpretability: we can observe behaviors without truly comprehending the underlying mechanisms.</p> <p>The core assumption deserves scrutiny: are we witnessing genuine evidence identification or sophisticated statistical pattern matching? The paper’s own results hint at limitations. Performance degrades on TriviaQA because models focus on evidence for only one of multiple possible answers, suggesting the attention patterns may reflect narrow statistical associations rather than comprehensive evidence evaluation.</p> <p>More concerning is the potential for spurious correlations masquerading as evidence detection. Recent research reveals that LLMs exhibit U-shaped attention bias, favoring tokens at context boundaries regardless of relevance. This “lost-in-the-middle” phenomenon suggests that what appears to be evidence identification might partially reflect positional biases rather than semantic understanding.</p> <p>The distinction between attention and actual relevance remains murky. While SelfElicit shows improved performance, this could result from exploiting existing model biases rather than genuinely enhancing evidence comprehension. The method’s adaptiveness in noisy contexts, selecting smaller evidence portions when distractors are present, might indicate sophisticated pattern recognition rather than true understanding of relevance.</p> <p>From a broader perspective, this work exemplifies both the promise and peril of post-hoc interpretability methods. We’re reverse-engineering behaviors we observe without necessarily understanding their foundations. The theoretical gap between statistical pattern matching and genuine comprehension remains a fundamental challenge in the field.</p> <p>Despite these concerns, the practical utility is undeniable. The method is computationally efficient and consistently improves performance across diverse tasks. However, we should remain cautious about anthropomorphizing these patterns, what looks like evidence identification might be elaborate statistical inference dressed in the language of understanding.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_35/">Literature Review: Universal Jailbreak Suffixes Are Strong Attention Hijackers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_21/">Literature Review: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_19/">Literature Review: DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_20/">Literature Review: Thinkless: LLM Learns When to Think</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/research_17/">Literature Review: Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jehyeok Yeon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>