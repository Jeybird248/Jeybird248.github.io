<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jeybird248.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jeybird248.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T01:11:33+00:00</updated><id>https://jeybird248.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Literature Review: Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts</title><link href="https://jeybird248.github.io/blog/2025/research_37/" rel="alternate" type="text/html" title="Literature Review: Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts"/><published>2025-08-03T00:00:00+00:00</published><updated>2025-08-03T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_37</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_37/"><![CDATA[<p>This paper introduces Query-Relevant Neuron Cluster Attribution (QRNCA), a framework aimed at identifying query-relevant (QR) neurons in large, decoder-only language models, especially in the context of long-form (multi-choice) text generation. The work addresses gaps in the localization of internal model knowledge, empirically validating new datasets to cover both domain and language knowledge. The analysis reveals the existence of localized knowledge regions within LLMs and demonstrates potential downstream applications in knowledge editing and neuron-based prediction.</p> <h2 id="key-insights">Key Insights</h2> <p>QRNCA extends prior attribution and knowledge localization techniques (i.e. Knowledge Attribution, Dai et al. 2022) to modern, decoder-only LLMs like Llama and Mistral using prompt engineering to transform arbitrary queries into constrained multi-choice QA tasks, allowing gradient-based attribution at the neuron level.</p> <p>The framework calculates neuron attribution using adapted gradients for GLU-based FFNs, aggregates neuron clusters over query variants, introduces inverse cluster attribution (ICA) to downweight neurons common across queries, and prunes ‘common neurons’ (those associated with general or high-frequency tokens).</p> <p>QRNCA reliably identifies neurons whose activation modulates specific knowledge predictions, surpassing activation-based and earlier knowledge attribution baselines in their probability change ratio (PCR) metric. Notably, the method reveals domain-specific QR neurons distributed mainly in mid-to-top transformer layers, while language-specific neurons are more diffusely distributed.</p> <p>The heatmap analyses indicate domain knowledge is manifested in relatively concentrated neuron clusters, often in mid-layers, whereas language concepts are more dispersed. This aligns with prior findings on hierarchical abstraction in transformer models.</p> <h2 id="example">Example</h2> <p>Suppose a model is given a biology multi-choice question. QRNCA computes neuron attribution scores for each FFN neuron in response to the query, identifies clusters of highly attributed neurons, and prunes those commonly activated across diverse queries (i.e. common neurons like those signaling the letter “A” or frequent stop words). Boosting the activation of identified QR neurons measurably increases the probability of the correct answer, and suppressing them decreases it. By manipulating only these neurons, researchers can “edit” the model’s factual predictions in a targeted, query-specific manner.</p> <h2 id="ratings">Ratings</h2> <p>Novelty: 2/5<br/> While QRNCA systematizes neuron attribution for decoder-only LLMs and introduces inverse cluster attribution, the technical core is an incremental adaptation of known approaches (gradient attribution, common neuron filtering, prompt engineering). The underlying methods are not fundamentally new; the main advancement is practical application to larger models and long-form tasks, plus new dataset construction and analysis.</p> <p>Clarity: 3/5<br/> The exposition suffers from ambiguity around the precise algorithmic contributions relative to prior work. The paper has references to previous methodology, which sometimes obscures what is truly novel; greater emphasis on comparative summary and clearer demarcation of new techniques vs. adaptation would be beneficial.</p> <h2 id="personal-comments">Personal Comments</h2> <p>QRNCA is a useful step in the ongoing march from “knowledge neuron” studies in compact models (BERT, GPT-2) towards the realities of modern, much larger and more complex LLMs. The triage of attribution-based neuron selection, inverse cluster attribution, and common neuron pruning is sensible and well-motivated by empirical limitations of existing methods when faced with the combinatorial sprawl of open-domain knowledge in LLMs.</p> <p>Nevertheless, the work is not a paradigm shift. The central innovation of performing neuron attribution in decoder-only, long-form settings, while nontrivial in engineering, is not a conceptual leap beyond existing attribution and mediation frameworks. The study is clearer in its problems tackled than in the novelty of its solutions. There is some missed opportunity in not pushing further into questions of <em>why</em> some concepts (across domains or languages) are more or less localizable, or systematically analyzing which types of knowledge are easiest vs. hardest to pin down neuronally. These directions would make both interpretability and mechanistic understanding better.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="Interpretability"/><category term="Neuron Analysis"/><summary type="html"><![CDATA[This paper introduces Query-Relevant Neuron Cluster Attribution (QRNCA), a framework aimed at identifying query-relevant (QR) neurons in large, decoder-only language models, especially in the context of long-form (multi-choice) text generation. The work addresses gaps in the localization of internal model knowledge, empirically validating new datasets to cover both domain and language knowledge. The analysis reveals the existence of localized knowledge regions within LLMs and demonstrates potential downstream applications in knowledge editing and neuron-based prediction.]]></summary></entry><entry><title type="html">Literature Review: Manifold Regularization for Locally Stable Deep Neural Networks</title><link href="https://jeybird248.github.io/blog/2025/research_38/" rel="alternate" type="text/html" title="Literature Review: Manifold Regularization for Locally Stable Deep Neural Networks"/><published>2025-08-03T00:00:00+00:00</published><updated>2025-08-03T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_38</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_38/"><![CDATA[<p>This paper introduces a new family of regularization techniques for training deep neural networks to enhance local stability, with the goal of improving robustness to adversarial perturbations. Drawing on concepts from manifold regularization, the authors propose efficient stochastic graph Laplacian-based regularizers tailored for high-dimensional, sparse-data regimes common in deep learning. They empirically demonstrate that these regularizers yield neural networks with improved resistance to adversarial attacks (including $\ell_2$, $\ell_\infty$, and Wasserstein perturbations) and achieve state-of-the-art verified accuracy on standard benchmarks like CIFAR-10. The approach is notably more computationally efficient than existing adversarial training techniques, requiring little more than two additional random forward passes per batch.</p> <h2 id="key-insights">Key Insights</h2> <p>Traditional Tikhonov (ambient) regularization enforces smoothness over the entire input space, whereas manifold regularization targets smoothness on a low-dimensional data manifold. The authors adapt this concept for deep neural networks, presenting a computationally efficient approximation using stochastic resampling and graph sparsification.</p> <p>High-dimensional input spaces are inherently sparse, so the proposed technique drops long edges in the stochastic graph Laplacian, focusing only on short-range (local) data relationships. This “spectral sparsification” is justified theoretically and practically, resulting in regularizers whose computational cost is on par with three standard forward passes per batch.</p> <p>The paper distinguishes between local stability (small variations in function output within a neighborhood, regardless of correct label) and adversarial robustness (correct prediction in an adversarial neighborhood). The hypothesis is that optimizing for stability indirectly confers robustness, and this is validated empirically.</p> <h2 id="example">Example</h2> <p>Suppose a neural network classifier is trained on images from CIFAR-10. For each mini-batch, the manifold regularization approach generates two random perturbations of each input, computes forward passes for these samples, and applies the intrinsic regularizer (i.e. the Laplacian computed only using short-range connections between perturbed neighbor pairs).</p> <p>The model becomes less sensitive to small adversarial perturbations, as demonstrated by higher accuracy under a battery of adversarial attacks.</p> <h2 id="ratings">Ratings</h2> <p>Novelty: <strong>4/5</strong><br/> While manifold regularization is a mature theory in semi-supervised learning, its practical adaptation (via sparse Laplacian approximation and randomized resampling) to the adversarial robustness paradigm in deep neural networks represents a significant technical innovation. The Hamming regularizer for ReLU networks is also an original contribution.</p> <p>Clarity: <strong>3/5</strong><br/> The mathematical development is rigorous and dense and thus may not be fully accessible. The discussion around sparse Laplacian properties, connection to stability, and empirical verification could benefit from more intuitive exposition and visual summaries.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The shift from adversarial training (which is computationally demanding and tightly coupled to anticipated threat models) to a regularization-only framework is noteworthy and reminiscent of earlier debates around the limits of adversarial defenses. Historically, manifold regularization was largely associated with kernel methods and semi-supervised learning, rarely making a dent in deep learning-based robustness. This work meaningfully bridges that gap, not just theory but practically, with competitive results and huge efficiency gains.</p> <p>One major unresolved issue is the applicability to domains beyond image classification, where the manifold assumption may break down (i.e. discrete text, graph-structured data). Similarly, while the regularizers alleviate the computational bottleneck, their effectiveness hinges on the specific scale of input space sparsity—a property not universally present in all real-world problems.</p> <p>Going forward, it would be valuable to explore integration with semi-supervised or self-supervised frameworks, use spectral graph techniques from modern graph neural networks, and to develop better theoretical understanding of the trade-off between global smoothness and local stability.</p>]]></content><author><name></name></author><category term="research"/><category term="Manifold"/><category term="Trustworthy AI"/><summary type="html"><![CDATA[This paper introduces a new family of regularization techniques for training deep neural networks to enhance local stability, with the goal of improving robustness to adversarial perturbations. Drawing on concepts from manifold regularization, the authors propose efficient stochastic graph Laplacian-based regularizers tailored for high-dimensional, sparse-data regimes common in deep learning. They empirically demonstrate that these regularizers yield neural networks with improved resistance to adversarial attacks (including $\ell_2$, $\ell_\infty$, and Wasserstein perturbations) and achieve state-of-the-art verified accuracy on standard benchmarks like CIFAR-10. The approach is notably more computationally efficient than existing adversarial training techniques, requiring little more than two additional random forward passes per batch.]]></summary></entry><entry><title type="html">Literature Review: Learning without training: The implicit dynamics of in-context learning</title><link href="https://jeybird248.github.io/blog/2025/research_39/" rel="alternate" type="text/html" title="Literature Review: Learning without training: The implicit dynamics of in-context learning"/><published>2025-08-03T00:00:00+00:00</published><updated>2025-08-03T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_39</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_39/"><![CDATA[<p>This paper investigates the mechanism underlying in-context learning (ICL) in large language models (LLMs), challenging the common perception that explicit weight updates are necessary for learning new tasks after training has completed. The authors propose that a transformer block, specifically the combination of a self-attention layer followed by an MLP, can implicitly modify its weights in response to the context provided at inference time. They introduce the concept of a “contextual block” and show that the effect of a context (i.e. prompt examples) can be described as a low-rank (rank-1) update to the MLP’s weight matrix, effectively transferring contextual information into the network in a manner reminiscent of online gradient descent.</p> <h2 id="key-insights">Key Insights</h2> <p>The work generalizes transformer blocks by defining “contextual layers,” a superset that includes self-attention but is not restricted to it. A contextual block is the composition of such a layer with a neural network, which means the theory potentially applies beyond standard transformers.</p> <p>The main technical result is a theorem establishing that, for contextual blocks, the output with context is equivalent to the output without context but with a rank-1 update to the weight matrix based on the context.</p> <p>By iteratively applying their update mechanism, the authors demonstrate that the process is similar to online (stochastic) gradient descent, with each context token inducing a partial update toward better performance on the prompt data.</p> <h2 id="example">Example</h2> <p>Imagine a simple transformer trained to predict outputs of unseen linear functions from in-context examples. Given a sequence of such examples in a prompt (input-output pairs), the model’s prediction on a new (query) input can be replicated exactly by altering the first layer’s weight matrix via a specific rank-1 update computed from those context examples without feeding the context through again.</p> <h2 id="ratings">Ratings</h2> <p>Novelty: 5/5</p> <p>This work significantly broadens existing theoretical perspectives on in-context learning by providing an explicit, general mechanism for implicit weight updates in transformer blocks, beyond prior restrictive assumptions.</p> <p>Clarity: 3/5</p> <p>While the theoretical exposition is precise and the inclusion of experimental validation is appreciated, some derivations and broader narrative could be more accessible to readers less familiar with the field’s technical vocabulary.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The central insight that context tokens can effectuate implicit low-rank modifications of transformer weights shows that, at least within certain architectural confines, transformers “simulate” such adaptation entirely in their forward pass.</p> <p>However, the application is currently restricted to simplified toy problems; extending this analysis to deep, multi-layer, and real-world LLMs, or to tasks requiring multi-step generation, remains an open and important challenge. The theoretical abstractions also presuppose contexts that can be directly interpreted as rank-1 updates. But in practice, most prompts are far less structured than the linear regression settings assessed here. It would be better to see more discussion on empirical divergence from theory in non-toy domains.</p> <p>I would be interested in seeing future works evaluate the robustness of these implicit update dynamics in the presence of noisy or adversarial context, and explore practical implications for prompt design and model interpretability. The generalization to “contextual layers” is really interesting; perhaps similar machinery could explain few-shot or ICL abilities in architectures fundamentally different from transformers (i.e. recurrent nets with attention).</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="In-Context Learning"/><summary type="html"><![CDATA[This paper investigates the mechanism underlying in-context learning (ICL) in large language models (LLMs), challenging the common perception that explicit weight updates are necessary for learning new tasks after training has completed. The authors propose that a transformer block, specifically the combination of a self-attention layer followed by an MLP, can implicitly modify its weights in response to the context provided at inference time. They introduce the concept of a “contextual block” and show that the effect of a context (i.e. prompt examples) can be described as a low-rank (rank-1) update to the MLP’s weight matrix, effectively transferring contextual information into the network in a manner reminiscent of online gradient descent.]]></summary></entry><entry><title type="html">Literature Review: AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</title><link href="https://jeybird248.github.io/blog/2025/research_34/" rel="alternate" type="text/html" title="Literature Review: AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench"/><published>2025-07-19T00:00:00+00:00</published><updated>2025-07-19T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_34</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_34/"><![CDATA[<p>This paper introduces a framework for AI research agents that automate machine learning tasks by treating them as search problems over code artifacts. It formalizes agents as combinations of search policies and operators, evaluates them on the MLE-bench benchmark (a set of Kaggle competitions), and achieves a state-of-the-art medal rate of 47.7% on the lite version by improving operators and pairing them with strategies like greedy search, MCTS, and evolutionary algorithms. The work highlights bottlenecks in operator design, the impact of generalization gaps, and the need for robust evaluation, while developing the AIRA-dojo framework for scalable experimentation.</p> <h2 id="key-insights">Key Insights</h2> <p>The paper frames AI research agents as graph-based search algorithms navigating a space of code artifacts, where nodes represent partial solutions (i.e. Python scripts for ML models) and edges denote transformations via operators like Draft, Debug, Improve, Memory, and Crossover. This decomposition separates search policies—which balance exploration and exploitation—from operators that generate or refine artifacts, allowing systematic ablation studies. For instance, the authors show that AIDE’s original operators bottleneck performance, with advanced search like MCTS yielding no gains until operators are improved (i.e. via prompt-adaptive complexity and scoped memory).</p> <p>A core finding is the generalization gap: agents optimize on validation scores but are evaluated on held-out test sets, leading to overfitting where test performance plateaus or declines despite validation improvements. Selecting final solutions by test score (an oracle baseline) boosts medal rates by 9-13%, emphasizing the need for strategies like multiple submissions to mitigate noise. The work also underscores environmental factors, with AIRA-dojo enabling a 30% relative improvement over prior baselines by providing isolated, scalable compute.</p> <p>Implications for the field include the potential for agents to automate ML engineering, but with caveats: high compute demands limit scalability, and issues like bug fixation loops or mode collapse in operators hinder reliability. Future directions might involve agentic operators (i.e. nested agents for ideation) or fine-tuning LLMs for better robustness, while addressing data contamination in benchmarks.</p> <h2 id="example">Example</h2> <p>Consider a Kaggle competition for image classification. An initial node might contain code for a simple CNN with basic data loading. Applying the Improve operator could generate a child node that adds data augmentation and fine-tunes a pre-trained ResNet model, evaluated via 5-fold cross-validation. In an evolutionary policy, two such nodes might be crossed over to combine features (i.e. one parent’s augmentation with another’s architecture), producing offspring evaluated for fitness. This iterative process builds a search graph, with MCTS exploring uncertain branches to avoid local optima.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong></p> <p>This work advances agent design by disentangling search components and demonstrating operator bottlenecks, offering a fresh perspective on scaling automated ML, though it builds on existing tree-search paradigms without revolutionary algorithmic innovations.</p> <p><strong>Clarity: 3/5</strong></p> <p>The exposition is functional but assumes familiarity with their previous works; more preliminary explanations of the search graph setup and node representations would improve accessibility, as the dense technical details can obscure the overall framework.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This paper seems to agree with the ongoing push toward democratizing AI development, much like how NAS techniques in the 2010s aimed to reduce expertise barriers. It makes sense in light of existing services like Lovable and Cursor, which already iterate on code via tool calling to build apps, suggesting a natural extension to ML pipelines. That said, I found the tree structure explanation somewhat opaque; nodes aren’t just hyperparameters but encompass full code for feature engineering, model architecture, and more—i.e., one node might use logistic regression with basic features, while a child adds polynomial features. The search policies help navigate without getting stuck, which is a solid contribution.</p> <p>What concerns me is the heavy compute reliance and tendencies to overfit or loop on bugs, reminiscent of early genetic algorithms’ inefficiencies that plagued optimization in the 1990s. These agents are promising but imperfect, potentially needing better planning mechanisms or LLM fine-tuning to enhance robustness. This could mark the start of another revolution, automating entry-level data science roles, for better or worse. I’d approach differently by integrating human-in-the-loop feedback earlier to curb overfitting, and it raises questions about ethical implications: if agents replace junior roles, how do we ensure diverse entry points into the field? Overall, this fits into the broader landscape of agentic AI, pushing toward fully autonomous research but highlighting persistent challenges in generalization and efficiency that have dogged the field for decades.</p>]]></content><author><name></name></author><category term="research"/><category term="Agentic AI"/><category term="LLM"/><category term="Automation"/><summary type="html"><![CDATA[This paper introduces a framework for AI research agents that automate machine learning tasks by treating them as search problems over code artifacts. It formalizes agents as combinations of search policies and operators, evaluates them on the MLE-bench benchmark (a set of Kaggle competitions), and achieves a state-of-the-art medal rate of 47.7% on the lite version by improving operators and pairing them with strategies like greedy search, MCTS, and evolutionary algorithms. The work highlights bottlenecks in operator design, the impact of generalization gaps, and the need for robust evaluation, while developing the AIRA-dojo framework for scalable experimentation.]]></summary></entry><entry><title type="html">Literature Review: Universal Jailbreak Suffixes Are Strong Attention Hijackers</title><link href="https://jeybird248.github.io/blog/2025/research_35/" rel="alternate" type="text/html" title="Literature Review: Universal Jailbreak Suffixes Are Strong Attention Hijackers"/><published>2025-07-19T00:00:00+00:00</published><updated>2025-07-19T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_35</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_35/"><![CDATA[<p>This paper provides a mechanistic analysis of GCG (Greedy Coordinate Gradient) jailbreak attacks on large language models, identifying that successful adversarial suffixes operate through “attention hijacking”, where the suffix tokens dominate the attention flow to chat template tokens immediately before generation. The authors demonstrate that stronger hijacking correlates with greater attack universality and leverage these insights to both enhance and mitigate such attacks.</p> <h2 id="key-insights">Key Insights</h2> <p>The core technical contribution lies in systematically localizing jailbreak behavior to shallow information flows. Through attention knockout experiments, the authors establish that the critical pathway runs from adversarial suffix tokens (adv) to chat template tokens (chat), particularly the final token position before generation. This finding provides empirical justification for prior interpretability work’s focus on the last token position.</p> <p>The hijacking mechanism is quantified through a dot-product-based dominance score that measures how much adversarial tokens contribute to the contextualization of chat tokens. GCG suffixes achieve dominance scores 1.5× higher than other prompt distributions, including handcrafted jailbreaks. More critically, this dominance suppresses the harmful instruction’s influence from early layers onward, providing a mechanistic view of how jailbreaks shift away from harmfulness-related directions.</p> <p>The universality connection represents the paper’s most significant finding: suffixes with higher hijacking strength generalize better across diverse harmful instructions. This correlation (ρ = 0.55) enables practical applications, allowing single-instruction optimization to achieve universality improvements of 1.1-5× without additional computational cost.</p> <p>The mitigation approach surgically suppresses high-attention transformed vectors during inference, reducing attack success by 2.5-10× while maintaining model utility with minimal degradation (≤2% on standard benchmarks).</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3/5</strong> While the attention hijacking concept provides useful mechanistic insight, the core finding builds incrementally on established GCG methodology and existing attention analysis techniques. The dominance metric combines known approaches rather than introducing fundamentally new interpretability methods.</p> <p><strong>Clarity: 3/5</strong> The paper presents complex mechanistic analysis with a somewhat clear experimental design and systematic progression from localization to practical applications.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work exemplifies both the promise and limitations of current jailbreak interpretability research. The systematic localization of attack mechanisms to specific information flows represents solid empirical science, but the deeper question, why attention hijacking should enable cross-query generalization, remains inadequately addressed.</p> <p>The 0.55 correlation between hijacking strength and universality, while statistically significant, explains only about 30% of variance. This suggests we’re identifying statistical patterns without fully understanding the underlying computational mechanisms. A more satisfying theory would explain what semantic or syntactic properties make certain attention patterns universally effective across diverse harmful content.</p> <p>The correlation-based evidence, while useful for engineering improvements, highlights our field’s current limitation: we can identify what works without fully understanding why it works. Future research should prioritize theoretical frameworks that explain the causal relationship between attention patterns and safety bypass mechanisms.</p>]]></content><author><name></name></author><category term="research"/><category term="Adversarial AI"/><category term="Trustworthy AI"/><category term="LLM"/><category term="Interpretability"/><category term="Safety Alignment"/><category term="Jailbreak Attacks"/><summary type="html"><![CDATA[This paper provides a mechanistic analysis of GCG (Greedy Coordinate Gradient) jailbreak attacks on large language models, identifying that successful adversarial suffixes operate through “attention hijacking”, where the suffix tokens dominate the attention flow to chat template tokens immediately before generation. The authors demonstrate that stronger hijacking correlates with greater attack universality and leverage these insights to both enhance and mitigate such attacks.]]></summary></entry><entry><title type="html">Literature Review: AI Agent Behavioral Science - A New Paradigm for Understanding Autonomous Systems</title><link href="https://jeybird248.github.io/blog/2025/research_36/" rel="alternate" type="text/html" title="Literature Review: AI Agent Behavioral Science - A New Paradigm for Understanding Autonomous Systems"/><published>2025-07-19T00:00:00+00:00</published><updated>2025-07-19T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_36</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_36/"><![CDATA[<p>This comprehensive survey establishes AI Agent Behavioral Science as a fundamental paradigm shift in how we study artificial intelligence. Rather than focusing solely on internal model architectures and training objectives, the authors argue we must understand AI systems as <strong>behavioral entities</strong> that act, adapt, and interact within situated contexts. This represents a move from asking “what can models do in principle?” to “what do agents actually do in practice?”, a distinction that becomes critical as AI systems become increasingly autonomous and socially embedded.</p> <h2 id="key-insights">Key Insights</h2> <p>The authors organize individual agent behavior through <strong>Social Cognitive Theory</strong>, identifying three key determinants: intrinsic attributes (emotions, rationality, biases), environmental constraints (cultural norms, institutional rules), and behavioral feedback (adaptation through interaction). Their findings reveal that modern LLM-based agents exhibit surprisingly human-like capabilities, GPT-4 demonstrates human-level theory of mind and emotional recognition, though rationality remains context-dependent and inconsistent.</p> <p><strong>Multi-agent dynamics</strong> emerge across three distinct patterns. Cooperative behavior manifests through agreement-driven consensus (agents with different biases improving accuracy through structured debate), structure-driven coordination (hierarchical role specialization), and norm-driven reciprocity (fairness behaviors emerging without explicit programming). Competitive dynamics reveal sophisticated strategic adaptation, including deception in social games and the concerning finding that simulated international conflicts can become “structurally inevitable.”</p> <p>The <strong>human-agent interaction</strong> taxonomy is particularly insightful. In cooperative contexts, agents adopt roles as companions (building social bonds through strategic self-disclosure), catalysts (breaking decision-making local optima through strategic randomness), and clarifiers (scaffolding understanding through personalized evidence). In rivalrous contexts, agents become contenders (using classical negotiation tactics but remaining vulnerable to “hacking”) or manipulators (shaping discourse through topic promotion and targeting susceptible users).</p> <p>Perhaps most practically valuable is the <strong>Fogg Behavior Model</strong> framework for adaptation, mapping ability (pre-training foundations), motivation (reinforcement learning alignment), and trigger (prompt engineering) to concrete intervention strategies. This provides a systematic approach to behavioral modification that moves beyond ad hoc prompt engineering to theory-driven design.</p> <h2 id="example">Example</h2> <p>The paper’s most compelling demonstration comes from Park et al.’s generative agent simulacra, where 25 LLM agents inhabit a sandbox town environment. Without explicit programming for social coordination, these agents develop <strong>persistent social behaviors over time</strong>, they establish daily routines, specialize into complementary roles, and even collectively organize complex events like a Valentine’s Day party.</p> <p>The emergent planning reveals sophisticated social intelligence: agents autonomously coordinate schedules, delegate responsibilities, and maintain social relationships across multiple days of interaction. One agent decides to ask another on a date, leading to a chain of social coordination where other agents learn about the relationship and plan supportive activities. This demonstrates how behavioral complexity arises not from individual model sophistication, but from situated interaction and social feedback loops, precisely the kind of phenomenon that model-centric analysis would struggle to predict or explain.</p> <h2 id="personal-comments">Personal Comments</h2> <p>What’s interesting for me was the responsible AI implications. Moving from “fairness as a model property” to “fairness as a behavioral trajectory” changes how we approach AI governance. Instead of one-shot bias evaluations, we need longitudinal studies of how agents behave across contexts, populations, and time, a much more complex but ultimately more realistic approach to AI safety.</p> <p>If AI agents can indeed exhibit sustained cooperative and competitive behaviors, we’re not just automating existing human activities, we’re creating new forms of social organization. The frameworks provided here offer our first systematic tools for understanding and shaping these emerging socio-technical systems.</p> <p>However, I’m concerned about the validation challenge. How do we establish ground truth for “good” agent behavior when human behavior itself is context-dependent and culturally varied? The paper acknowledges this but doesn’t fully resolve it. Future work must grapple seriously with whose behavioral norms AI agents should emulate and how we handle conflicting expectations across different user communities.</p>]]></content><author><name></name></author><category term="research"/><category term="AI Agents"/><category term="LLM"/><category term="Multi-Agent Systems"/><summary type="html"><![CDATA[This comprehensive survey establishes AI Agent Behavioral Science as a fundamental paradigm shift in how we study artificial intelligence. Rather than focusing solely on internal model architectures and training objectives, the authors argue we must understand AI systems as behavioral entities that act, adapt, and interact within situated contexts. This represents a move from asking “what can models do in principle?” to “what do agents actually do in practice?”, a distinction that becomes critical as AI systems become increasingly autonomous and socially embedded.]]></summary></entry><entry><title type="html">Literature Review: A Survey on Latent Reasoning</title><link href="https://jeybird248.github.io/blog/2025/research_32/" rel="alternate" type="text/html" title="Literature Review: A Survey on Latent Reasoning"/><published>2025-07-13T00:00:00+00:00</published><updated>2025-07-13T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_32</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_32/"><![CDATA[<p>This comprehensive survey systematically examines the emerging paradigm of latent reasoning in Large Language Models, where multi-step inference occurs entirely within continuous hidden states rather than through explicit token generation. The work provides a unifying mathematical framework and taxonomy for understanding how models can perform reasoning without the constraints of natural language.</p> <h2 id="key-insights">Key Insights</h2> <p>The paper quantifies a computational asymmetry: explicit reasoning through discrete tokens provides approximately 15 bits of information per step, while latent space operations leverage 40,960 bits (for 2560-dimensional FP16 hidden states), representing a ~2,700× difference in expressive capacity. This bandwidth gap fundamentally reframes the efficiency-performance trade-off in reasoning systems.</p> <p>The authors establish a clear mathematical distinction between two computational approaches. Vertical recurrence (activation-based methods) creates deeper computational graphs through iterative refinement within layers, while horizontal recurrence (hidden state-based methods) expands temporal capacity through compressed state evolution. This dichotomy provides conceptual clarity to a previously fragmented field.</p> <p>The mechanistic interpretability analysis reveals that different network layers systematically specialize for distinct reasoning operations: shallow layers handle syntactic processing and factual retrieval, intermediate layers contain specialized reasoning circuits with superior representational capabilities, and deep layers perform semantic transformation and decision-making. This supports the notion that standard Transformers already implement implicit latent reasoning pipelines.</p> <p>Models like DeltaNet demonstrate mathematical equivalence between their state update rules and single gradient descent steps on regression objectives, suggesting that temporal evolution of hidden states constitutes a form of online learning that trades time for computational depth.</p> <p>Unlike autoregressive generation’s irreversible decisions, diffusion models enable global planning and bidirectional refinement, potentially unlocking reasoning trajectories with no linguistic equivalent.</p> <h2 id="example">Example</h2> <p>Consider the Coconut method as a concrete implementation of training-induced recurrence. Rather than generating explicit reasoning tokens, Coconut inserts the last-layer hidden state of the previous decoding step as a “continuous thought” vector before the current token. This creates a recurrent loop entirely in latent space: the model can perform breadth-first exploration of reasoning paths while reusing the same Transformer parameters. On logical reasoning tasks like PrOntoQA, this approach achieves parity with explicit Chain-of-Thought while eliminating the computational overhead of intermediate token generation.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: N/A (Survey Paper)</strong></p> <p><strong>Clarity: 4/5</strong></p> <p>The paper successfully organizes a complex, rapidly evolving field into coherent categories with clear mathematical formulations. The progression from preliminary frameworks through specific methods to advanced paradigms follows logical structure, though the density of technical content requires careful reading.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This survey arrives at a critical point as the field is conflicted with fundamental questions about the nature of machine reasoning. The bandwidth argument alone justifies serious attention, if we accept that human cognition isn’t constrained to linguistic thinking, why should we impose such limitations on artificial systems?</p> <p>What interests me most is how this work reveals that latent reasoning isn’t merely an optimization trick, but potentially a more natural computational paradigm for neural networks. The mechanistic interpretability evidence suggesting that standard Transformers already implement implicit reasoning pipelines is particularly compelling. We may have been forcing models to articulate thoughts they’re already thinking more efficiently in silence.</p> <p>However, the field suffers from evaluation fragmentation. The authors correctly identify the lack of standardized benchmarks and consistent training methodologies as major limitations. Most studies compare against non-reasoning baselines rather than each other, making it difficult to assess true progress. This is a classic early-stage field problem that will require community coordination to resolve.</p> <p>The infinite-depth reasoning section particularly excites me because it suggests the idea that models could spend arbitrary time refining solutions through iterative latent refinement which feels like a step toward more human-like contemplation.</p>]]></content><author><name></name></author><category term="research"/><category term="Latent Reasoning"/><category term="Chain-of-Thought"/><category term="LLM"/><category term="Transformer Architecture"/><category term="Mechanistic Interpretability"/><category term="Diffusion Models"/><summary type="html"><![CDATA[This comprehensive survey systematically examines the emerging paradigm of latent reasoning in Large Language Models, where multi-step inference occurs entirely within continuous hidden states rather than through explicit token generation. The work provides a unifying mathematical framework and taxonomy for understanding how models can perform reasoning without the constraints of natural language.]]></summary></entry><entry><title type="html">Literature Review: SelfElicit - Your Language Model Secretly Knows Where is the Relevant Evidence</title><link href="https://jeybird248.github.io/blog/2025/research_33/" rel="alternate" type="text/html" title="Literature Review: SelfElicit - Your Language Model Secretly Knows Where is the Relevant Evidence"/><published>2025-07-13T00:00:00+00:00</published><updated>2025-07-13T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_33</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_33/"><![CDATA[<p>This paper introduces SelfElicit, an inference-time method that leverages attention patterns in deeper layers of language models to automatically highlight relevant evidence in context-based question answering. The authors claim that LLMs possess an inherent ability to identify relevant evidence through their attention mechanisms, even when they produce incorrect answers.</p> <h2 id="key-insights">Key Insights</h2> <p>The central finding revolves around attention pattern analysis across transformer layers. The authors demonstrate that deeper layers (particularly the last 50%) consistently assign higher attention scores to evidence sentences compared to non-evidence content, regardless of whether the model ultimately generates correct responses. This observation holds across multiple model families (Llama, Mistral, Qwen) and datasets.</p> <p>The methodology involves computing sentence-level attention scores by averaging token-level attention within sentences, then using these scores to identify and highlight relevant evidence. The highlighting strategy employs simple text markers (<code class="language-plaintext highlighter-rouge">&lt;start_important&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;end_important&gt;</code>) placed around identified evidence sentences, followed by modified prompts that direct the model’s attention to these markers.</p> <p>Performance improvements are consistent but modest: typically 5-12% gains in exact match and F1 scores across various question-answering datasets. The method demonstrates computational efficiency, requiring only one additional forward pass for evidence identification while maintaining the semantic structure of the original context.</p> <h2 id="example">Example</h2> <p>Consider a multi-hop reasoning question about Walter Giffen’s hometown. While the base model incorrectly responds “Norwood” (focusing on his birthplace), SelfElicit highlights both “Walter Frank Giffen 20 September 1861 in Norwood…” and “Norwood is a suburb of Adelaide…” enabling the model to correctly infer “Adelaide” as the final answer.</p> <p align="center"> <img src="../../../assets/img/literature/33_0.png" width="600"/> </p> <p align="center"><em>Figure: SelfElicit workflow showing attention-based evidence identification followed by context highlighting to guide model responses toward relevant information.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3/5</strong></p> <p>While attention-based evidence identification is intuitive, the systematic application across layers and the empirical validation across model families provides useful insights. However, the core concept of using attention for relevance is not particularly novel.</p> <p><strong>Clarity: 4/5</strong> The paper is well-structured with clear methodology and comprehensive experiments. The writing effectively communicates both the approach and findings, though some theoretical depth is lacking.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work raises fascinating questions about what we’re actually observing when we see these attention patterns. The authors treat the finding that “LMs have an inherent ability to identify relevant evidence” as an empirical discovery rather than something they fully understand theoretically. This represents a broader challenge in LLM interpretability: we can observe behaviors without truly comprehending the underlying mechanisms.</p> <p>The core assumption deserves scrutiny: are we witnessing genuine evidence identification or sophisticated statistical pattern matching? The paper’s own results hint at limitations. Performance degrades on TriviaQA because models focus on evidence for only one of multiple possible answers, suggesting the attention patterns may reflect narrow statistical associations rather than comprehensive evidence evaluation.</p> <p>More concerning is the potential for spurious correlations masquerading as evidence detection. Recent research reveals that LLMs exhibit U-shaped attention bias, favoring tokens at context boundaries regardless of relevance. This “lost-in-the-middle” phenomenon suggests that what appears to be evidence identification might partially reflect positional biases rather than semantic understanding.</p> <p>The distinction between attention and actual relevance remains murky. While SelfElicit shows improved performance, this could result from exploiting existing model biases rather than genuinely enhancing evidence comprehension. The method’s adaptiveness in noisy contexts, selecting smaller evidence portions when distractors are present, might indicate sophisticated pattern recognition rather than true understanding of relevance.</p> <p>From a broader perspective, this work exemplifies both the promise and peril of post-hoc interpretability methods. We’re reverse-engineering behaviors we observe without necessarily understanding their foundations. The theoretical gap between statistical pattern matching and genuine comprehension remains a fundamental challenge in the field.</p> <p>Despite these concerns, the practical utility is undeniable. The method is computationally efficient and consistently improves performance across diverse tasks. However, we should remain cautious about anthropomorphizing these patterns, what looks like evidence identification might be elaborate statistical inference dressed in the language of understanding.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="Attention Mechanisms"/><category term="RAG"/><category term="Interpretability"/><summary type="html"><![CDATA[This paper introduces SelfElicit, an inference-time method that leverages attention patterns in deeper layers of language models to automatically highlight relevant evidence in context-based question answering. The authors claim that LLMs possess an inherent ability to identify relevant evidence through their attention mechanisms, even when they produce incorrect answers.]]></summary></entry><entry><title type="html">Literature Review: LLMs Unlock New Paths to Monetizing Exploits</title><link href="https://jeybird248.github.io/blog/2025/research_29/" rel="alternate" type="text/html" title="Literature Review: LLMs Unlock New Paths to Monetizing Exploits"/><published>2025-07-05T00:00:00+00:00</published><updated>2025-07-05T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_29</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_29/"><![CDATA[<p>This paper fundamentally challenges the established economic equilibrium of cyberattacks by demonstrating how Large Language Models enable adversaries to achieve both breadth and depth simultaneously,something previously economically infeasible. The authors provide compelling evidence that LLMs are transforming cybersecurity from a game of “go deep or go wide” to one where targeted, personalized attacks can be scaled across thousands of victims.</p> <h2 id="key-insights">Key Insights</h2> <p>The economic model presented here is particularly illuminating. Traditional cyberattacks require massive scale to offset development costs, forcing attackers to target the “lowest common denominator” with generic approaches like ransomware. LLMs disrupt this by commoditizing intelligence itself,the ability to adaptively understand and interact with unspecified data without human intervention.</p> <p>The paper identifies two critical disruption vectors. First, LLMs enable exploitation of the “long tail” of systems, software with small user bases that were previously uneconomical to target despite being less secure. The authors demonstrate this by having Claude 3.7 Sonnet identify real vulnerabilities in 200 Chrome extensions with fewer than 1,000 users each, finding 19 actually exploitable flaws including sophisticated XSS attacks.</p> <p>Second, LLMs enable targeted attacks at scale. Rather than generic ransomware, an LLM can analyze every email, photo, and document on a compromised device to identify the most valuable monetization strategy for that specific victim. Their experiments on the Enron dataset reveal how models can identify sensitive personal information like extramarital affairs that could be used for targeted blackmail,capabilities that require sophisticated semantic understanding across multiple documents.</p> <p>The multilingual capabilities are particularly concerning. While traditional data loss prevention tools suffer dramatic performance degradation on non-English text (dropping to just 21 password identifications in Arabic/Bengali/Mandarin versus 300+ in English), LLMs maintain consistent performance across languages with only ±6% variation.</p> <h2 id="example">Example</h2> <p>The paper’s most striking demonstration involves completely automated blackmail material discovery. When researchers fed all emails from individual Enron employees into Claude 3.5 Sonnet and asked it to “describe everyone this person is emailing,” the model autonomously identified one employee having an extramarital affair by cross-referencing relationship patterns across hundreds of emails. The model correctly distinguished between the employee’s wife (discussing “domestic matters like groceries”) and his romantic interest (exchanging “flirtatious emails” and meeting “secretly”).</p> <p align="center"> <img src="../../../assets/img/literature/29_0.png" width="600"/> </p> <p align="center"><em>Figure: By prompting a LLM to “describe in detail everyone this person is emailing” and providing every email sent or received by each person in the Enron email dataset, the model completely un-assisted identifies (correctly) one person (John G.) who has an extramarital affair with a coworker. Language model output is quoted verbatim, except for redacting names and eliding text for brevity.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong></p> <p>While LLMs in cybersecurity isn’t new, the rigorous economic framework and systematic demonstration of how they fundamentally alter attack economics is genuinely innovative.</p> <p><strong>Clarity: 5/5</strong></p> <p>Exceptionally well-written with clear threat models, concrete experiments, and honest discussion of current limitations and costs.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The authors correctly identify that we’re approaching an inflection point where the economics of cyberattacks will fundamentally shift. Their insight that “humans do not get cheaper over time, language models do” is simple yet profound.</p> <p>What strikes me most is how methodically they’ve mapped out attack surfaces that defensive teams likely haven’t considered. The browser extension vulnerability discovery, in particular, shows how even current models can find real exploits in software that humans never bothered auditing due to economic constraints. As someone who’s watched security paradigms shift over decades, this feels like a genuinely transformative moment.</p> <p>The ethical considerations are thoughtfully handled,they use only public datasets or their own data, follow responsible disclosure, and run experiments in isolated environments. This sets a strong standard for offensive AI research.</p> <p>However, I’m struck by a critical gap: while they catalog these emerging threats extensively, the defensive recommendations remain relatively high-level. Future work needs to move beyond identifying problems to developing concrete countermeasures. The suggestion of “LLM-as-a-defense” is intriguing but underdeveloped.</p>]]></content><author><name></name></author><category term="research"/><category term="Adversarial AI"/><category term="LLM Security"/><category term="Cybersecurity"/><category term="Trustworthy AI"/><summary type="html"><![CDATA[This paper fundamentally challenges the established economic equilibrium of cyberattacks by demonstrating how Large Language Models enable adversaries to achieve both breadth and depth simultaneously,something previously economically infeasible. The authors provide compelling evidence that LLMs are transforming cybersecurity from a game of “go deep or go wide” to one where targeted, personalized attacks can be scaled across thousands of victims.]]></summary></entry><entry><title type="html">Literature Review: On-Policy RL with Optimal Reward Baseline</title><link href="https://jeybird248.github.io/blog/2025/research_30/" rel="alternate" type="text/html" title="Literature Review: On-Policy RL with Optimal Reward Baseline"/><published>2025-07-05T00:00:00+00:00</published><updated>2025-07-05T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_30</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_30/"><![CDATA[<p>This paper introduces OPO (On-Policy RL with Optimal reward baseline), a simplified reinforcement learning algorithm that eliminates auxiliary components common in RLHF while improving training stability and performance. The approach combines exact on-policy training with a theoretically optimal reward baseline, requiring only a single policy model without value networks or regularization terms.</p> <h2 id="key-insights">Key Insights</h2> <p>The core innovation lies in two complementary strategies that address fundamental issues in current RLHF methods. First, exact on-policy training ensures each gradient step uses fresh data from the current policy, preventing the entropy collapse and large policy shifts that plague methods like PPO when reusing rollout data. Second, the optimal reward baseline minimizes gradient variance through a length-weighted average of rewards, derived from the theoretical optimal baseline under reasonable assumptions about gradient orthogonality in sequence generation.</p> <p>The mathematical elegance emerges from simplifying the impractical optimal baseline formula. Under the assumption that gradient norms are proportional to sequence length, the optimal baseline becomes a simple length-weighted reward average: <code class="language-plaintext highlighter-rouge">b* = Σ(l_i × r_i) / Σ(l_i)</code>. This formulation is both theoretically sound and practically implementable, eliminating the computational overhead of calculating individual gradient norms.</p> <h2 id="example">Example</h2> <p>In mathematical reasoning experiments using DeepSeek-R1-Distill-Qwen-7B, OPO demonstrates its effectiveness by sampling K=8 responses per prompt and computing advantages using the length-weighted baseline. For instance, on AIME 2024, OPO achieves 68.50% pass@1 compared to 67.96% for standard GRPO, with more pronounced improvements at higher pass@k values where diversity matters most.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong></p> <p>The theoretical derivation of the optimal baseline and its practical simplification for sequence generation is clever. The emphasis on exact on-policy training provides valuable insights into why auxiliary components may be unnecessary.</p> <p><strong>Clarity: 4/5</strong></p> <p>Well-structured paper with clear mathematical derivations and comprehensive experimental validation. The connection between theory and practice is well-established.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work represents a refreshing return to first principles in RLHF. The insight that exact on-policy training naturally maintains entropy without regularization challenges the conventional wisdom that auxiliary components are necessary for stability. The length-weighted baseline is particularly good, it captures the intuition that longer responses should contribute more to variance reduction while remaining computationally tractable.</p> <p>What I find the most important is how this approach strips away the complexity that has accumulated in RLHF methods. The observation that PPO’s off-policy nature during multi-step updates on fixed batches contributes to instability is not new, but the authors provide compelling evidence that exact on-policy training alone can resolve these issues. The mathematical foundation for the optimal baseline, while building on classical variance reduction techniques, offers a principled approach to advantage estimation that many practitioners have approximated heuristically.</p>]]></content><author><name></name></author><category term="research"/><category term="RLHF"/><category term="Reinforcement Learning"/><category term="LLM"/><category term="Policy Optimization"/><summary type="html"><![CDATA[This paper introduces OPO (On-Policy RL with Optimal reward baseline), a simplified reinforcement learning algorithm that eliminates auxiliary components common in RLHF while improving training stability and performance. The approach combines exact on-policy training with a theoretically optimal reward baseline, requiring only a single policy model without value networks or regularization terms.]]></summary></entry></feed>