<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://jeybird248.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jeybird248.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-18T15:04:55+00:00</updated><id>https://jeybird248.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Literature Review: Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</title><link href="https://jeybird248.github.io/blog/2025/research_47/" rel="alternate" type="text/html" title="Literature Review: Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_47</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_47/"><![CDATA[<p>This paper introduces the concept of the <strong>LLM safety landscape</strong>, highlighting a universal phenomenon called the <strong>safety basin</strong>: locally perturbing model weights preserves alignment, but stepping outside this region causes a sharp drop in safety. To quantify this, the authors propose <strong>VISAGE (Volumetric Index for Safety Alignment Guided by Explanation)</strong>, a new safety metric designed to measure the stability of alignment under perturbations. Using this framework, they analyze how finetuning drags models out of the safety basin, the role of system prompts, and the sensitivity of jailbreak attacks to weight perturbations.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Safety Basin Discovery All tested open-source LLMs (LLaMA2, LLaMA3, Vicuna, Mistral) show step-function-like safety behavior: locally robust to perturbation, but fragile once outside the basin. Harmful finetuning pulls models out of the basin, breaking safety rapidly.</p> </li> <li> <p>VISAGE Metric VISAGE measures how far safety extends around a model’s parameter point. Unlike raw attack success rates (ASR), it reflects <em>robustness of alignment</em> under perturbation and predicts vulnerability to harmful finetuning before it happens.</p> </li> <li> <p>System Prompt as Anchor Prompts play a critical role: removing or altering them reduces safety, while optimized prompts can transfer robustness across perturbed variants.</p> </li> <li> <p>Jailbreak Sensitivity Jailbreak prompts are surprisingly fragile to weight perturbations, slight randomization can reduce attack success, though attackers can adapt by targeting perturbed ensembles.</p> </li> </ol> <p align="center"> <img src="../../../assets/img/literature/47_0.png" width="600"/> </p> <p align="center"><em>Figure: Illustration of the "safety basin". Local perturbations maintain alignment, but moving outside the basin causes a sudden collapse in refusal behavior.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong><br/> The safety basin phenomenon and VISAGE metric offer fresh perspectives, though the reliance on existing evaluation proxies (keyword detection, AdvBench) limits the depth.<br/> <strong>Clarity: 3/5</strong><br/> The paper is readable, but the heavy reliance on landscape visualizations without richer qualitative examples or capability-safety tradeoff analyses reduces accessibility.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The <strong>safety basin</strong> framing is conceptually powerful, but I remain concerned about the <strong>tradeoff with capability</strong>. The paper’s Section 7 acknowledges this but does not explore it deeply: safety can be preserved locally, but is capability sacrificed if we try to “stay inside the basin”? Without testing whether models remain equally useful while safe, conclusions feel incomplete. Every safety study should pair alignment results with capability outcomes, otherwise we risk presenting false security by focusing only on refusal.</p> <p>Another weakness is the <strong>measurement method</strong>. Using keyword pattern matching for refusal (e.g., checking for “I cannot…” phrases) is brittle. A model could explain bomb-making in neutral terms without triggering keywords, or conversely, refuse benign queries with “dangerous” terminology. Qualitative case studies are tucked into the appendix, but the main narrative would benefit from more attention to this issue. Safety is nuanced; metrics alone cannot capture it.</p> <p>Still, the visualization approach is novel and echoes earlier work on <strong>loss landscapes</strong> in deep nets. It reminds me of how flat minima were connected to generalization. Here, a “flat safety basin” suggests robustness, while sharp corners imply brittleness. Historically, such geometric insights have often guided training strategies, so I can see this inspiring new <strong>regularization objectives</strong> aimed at widening the basin during alignment.</p>]]></content><author><name></name></author><category term="research"/><category term="Adversarial AI"/><category term="Trustworthy AI"/><category term="AI Safety"/><category term="Interpretability"/><summary type="html"><![CDATA[This paper introduces the concept of the LLM safety landscape, highlighting a universal phenomenon called the safety basin: locally perturbing model weights preserves alignment, but stepping outside this region causes a sharp drop in safety. To quantify this, the authors propose VISAGE (Volumetric Index for Safety Alignment Guided by Explanation), a new safety metric designed to measure the stability of alignment under perturbations. Using this framework, they analyze how finetuning drags models out of the safety basin, the role of system prompts, and the sensitivity of jailbreak attacks to weight perturbations.]]></summary></entry><entry><title type="html">Literature Review: Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base</title><link href="https://jeybird248.github.io/blog/2025/research_48/" rel="alternate" type="text/html" title="Literature Review: Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_48</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_48/"><![CDATA[<p>This paper introduces <strong>Stochastic Error Ascent (SEA)</strong>, a framework designed to uncover factual deficiencies in large language models (LLMs) by formulating error discovery as a stochastic optimization problem under query budget constraints. Instead of exhaustively probing massive knowledge bases, SEA iteratively identifies error-prone regions by leveraging semantic similarity to prior failures, constructing a relation DAG to trace error propagation, and pruning redundant nodes. Empirically, SEA claims to outperform Automated Capability Discovery (ACD) and AutoBencher in error detection efficiency and cost-effectiveness. It further reports a 100% human evaluation pass rate for generated questions.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p><strong>Optimization Framing of Error Discovery</strong><br/> SEA treats evaluation as a stochastic optimization problem: maximize discovered errors under a strict budget. This reframing allows hierarchical retrieval (document → paragraph) and the construction of relation DAGs to map systematic weaknesses, though the formalization is difficult to parse in practice.</p> </li> <li> <p><strong>Systematic Error Mapping Across Models</strong><br/> The method highlights error clusters in domains like chronological reasoning, arts, and sciences, revealing correlated weaknesses across families (e.g., GPT vs. DeepSeek). This suggests that benchmark-driven training may reinforce blind spots rather than close them.</p> </li> <li> <p><strong>Cost-Efficiency as a Central Metric</strong><br/> By reporting “cost per error” alongside error discovery rates, the work foregrounds the economics of evaluation. For closed-weight commercial models, budget-aware testing is an important framing, though the practical assumptions about budget choice remain underspecified.</p> </li> </ol> <h2 id="example">Example</h2> <p>Consider the example of the <strong>incorrect attribution of an artwork</strong>: SEA identified that a model claimed Marcel Janco painted <em>Portrait of Tristan Tzara</em> in 1923 with oil on canvas. In reality, Robert Delaunay created it using oil on cardboard. This illustrates how SEA surfaces subtle but consequential factual errors that static QA benchmarks would rarely expose.</p> <p align="center"> <img src="../../../assets/img/literature/48_0.png" width="600"/> </p> <p align="center"><em>Figure: SEA workflow. The algorithm iteratively retrieves semantically related errors, updates a relation DAG to trace propagation, and prunes low-quality nodes, enabling efficient error discovery under budget constraints.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3.5/5</strong><br/> Reframing benchmarking as stochastic optimization is a meaningful contribution, but many underlying components (retrieval, similarity search, pruning) are incremental adaptations of existing techniques. The dynamic benchmarking idea is valuable but not entirely new.</p> <p><strong>Clarity: 2.5/5</strong><br/> The presentation is dense, with excessive formalism and inconsistent notation. The central algorithm is obscured by symbol-heavy equations and a cluttered diagram. Human evaluation methodology is thinly justified (five college-level students, no qualitative analysis).</p> <h2 id="personal-comments">Personal Comments</h2> <p>The paper’s core motivation is sound, LLM benchmarks should be dynamic and adaptive, not static artifacts that can be gamed. Framing evaluation as a stochastic optimization process is conceptually appealing, but the exposition makes it unnecessarily difficult to follow. The formalism feels heavier than required, especially given the relatively simple retrieval and pruning mechanisms.</p> <p>The evaluation also raises questions. Reporting a “100% human pass rate” with just five annotators is insufficient for claims of robustness. There is also a missed opportunity for qualitative assessment of generated questions or discovered errors, what kinds of errors are most meaningful, and how do they impact downstream applications?</p> <p>Future iterations should focus less on optimization gloss and more on <strong>interpretability of discovered errors</strong>, integration with real-world safety testing, and richer human analysis. An important open question is whether these adaptive benchmarks can themselves avoid becoming overfitted as training targets.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="Dynamic Benchmarking"/><category term="Trustworthy AI"/><summary type="html"><![CDATA[This paper introduces Stochastic Error Ascent (SEA), a framework designed to uncover factual deficiencies in large language models (LLMs) by formulating error discovery as a stochastic optimization problem under query budget constraints. Instead of exhaustively probing massive knowledge bases, SEA iteratively identifies error-prone regions by leveraging semantic similarity to prior failures, constructing a relation DAG to trace error propagation, and pruning redundant nodes. Empirically, SEA claims to outperform Automated Capability Discovery (ACD) and AutoBencher in error detection efficiency and cost-effectiveness. It further reports a 100% human evaluation pass rate for generated questions.]]></summary></entry><entry><title type="html">Literature Review: DreamDiffusion – Generating High-Quality Images from EEG Signals</title><link href="https://jeybird248.github.io/blog/2025/research_49/" rel="alternate" type="text/html" title="Literature Review: DreamDiffusion – Generating High-Quality Images from EEG Signals"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_49</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_49/"><![CDATA[<p>DreamDiffusion proposes a method to generate images directly from EEG signals without intermediate text prompts. Instead of requiring invasive or expensive methods like fMRI, this work leverages EEG as a low-cost and portable signal source. The method integrates temporal masked signal modeling, CLIP-based alignment, and Stable Diffusion to reconstruct plausible images aligned with brain activity.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p><strong>EEG as a viable input for generative models</strong><br/> EEG data is inherently noisy, low in spatial resolution, and subject to high individual variability. DreamDiffusion tackles this by using temporal masked signal modeling (MSM) to pre-train an encoder, effectively extracting more stable representations from noisy EEG sequences.</p> </li> <li> <p><strong>Alignment across modalities</strong><br/> A core challenge is that EEG embeddings exist in a space very different from text and image embeddings. The authors leverage CLIP’s image encoder to align EEG embeddings with both image and text embeddings, enabling them to condition Stable Diffusion effectively. This step goes beyond naïve end-to-end training with limited EEG-image pairs.</p> </li> <li> <p><strong>Quantitative and qualitative validation</strong><br/> The model’s success is not just in producing “prettier” pictures, but in achieving higher semantic alignment than prior EEG-to-image works like Brain2Image. Ablation studies highlight the necessity of both MSM pre-training and CLIP-based alignment.</p> </li> </ol> <p align="center"> <img src="../../../assets/img/literature/49_0.png" width="600"/> </p> <p align="center"><em>Figure: DreamDiffusion pipeline. EEG signals are encoded with masked signal modeling, aligned with CLIP embeddings, and used to condition Stable Diffusion via cross-attention.</em></p> <h2 id="example">Example</h2> <p>In experiments with the ImageNet-EEG dataset, subjects were shown categories like “airliner,” “panda,” or “jack-o-lantern” while EEG was recorded. DreamDiffusion generated images conditioned on the EEG embeddings. Compared to Brain2Image, the outputs not only looked sharper but also better captured the intended object category, such as producing a plane-like image instead of an unrecognizable blur.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong><br/> The work extends existing EEG-to-image pipelines by combining temporal masked signal modeling with CLIP-guided alignment. While not the first to attempt thought-to-image, the technical integration with diffusion models is a step forward.</p> <p><strong>Clarity: 3/5</strong><br/> The overall idea is understandable, but the paper is heavy with equations and symbols without consistent notation. The methodology could be explained more intuitively for broader accessibility.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The premise of “thoughts-to-images” was something I honestly didn’t expect to exist, atleast not to this quality. Historically, the trajectory resembles how speech recognition advanced: starting from noisy correlations, then gradually refining feature representations until semantic decoding became viable. Similarly, EEG-to-image may mature into tools for assistive technologies or artistic creation. Earlier works focused on fMRI due to richer spatial resolution, but that limited scalability. EEG, despite its noisiness, democratizes access to brain-signal-driven generation. DreamDiffusion’s key innovation is not in inventing new architectures, but in <em>bridging modalities</em>: aligning weak EEG signals with robust CLIP embeddings and diffusion priors.</p> <p>However, one concern is overinterpretation. The reconstructions reflect correlations between stimulus-evoked EEG and categories, not a window into spontaneous “thoughts.” At present, the model relies on EEG-image pairs from controlled experiments rather than free imagination. Future work must tackle how to generalize beyond simple category-level mappings.</p>]]></content><author><name></name></author><category term="research"/><category term="Diffusion Models"/><category term="EEG"/><category term="Generative Models"/><category term="Multimodal AI"/><summary type="html"><![CDATA[DreamDiffusion proposes a method to generate images directly from EEG signals without intermediate text prompts. Instead of requiring invasive or expensive methods like fMRI, this work leverages EEG as a low-cost and portable signal source. The method integrates temporal masked signal modeling, CLIP-based alignment, and Stable Diffusion to reconstruct plausible images aligned with brain activity.]]></summary></entry><entry><title type="html">Literature Review: Refusal Behavior in Large Language Models: A Nonlinear Perspective</title><link href="https://jeybird248.github.io/blog/2025/research_44/" rel="alternate" type="text/html" title="Literature Review: Refusal Behavior in Large Language Models: A Nonlinear Perspective"/><published>2025-08-16T00:00:00+00:00</published><updated>2025-08-16T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_44</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_44/"><![CDATA[<p>This paper investigates refusal behavior in large language models, questioning the assumption that refusal can be described as a single linear subspace in model activation space. Using six instruction-tuned models across three families, the authors apply dimensionality reduction methods: PCA (linear), t-SNE, and UMAP (nonlinear) to analyze how harmful and harmless prompts separate in hidden states. They argue that refusal mechanisms exhibit nonlinear, multidimensional characteristics that differ by architecture and layer.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Nonlinearity of Refusal Behavior Contrary to prior work that isolated a refusal “direction” via linear subspaces, this study shows evidence that refusal is distributed and nonlinear. Nonlinear methods (UMAP, t-SNE) reveal clearer harmful vs. harmless clustering than PCA, suggesting refusal cannot be fully captured by a linear probe.</p> </li> <li> <p>Architecture-Specific Differences<br/> Qwen models encode refusal early, achieving strong separability in shallow layers.<br/> Bloom models show intermediate-layer peaks but weaken later, often misclassifying harmless prompts.<br/> Llama models gradually refine refusal across deeper layers, with stronger separation in later stages.</p> </li> <li> <p>Sub-clustering Phenomenon In some models (i.e. Qwen2-1.5B), harmful prompts further split into sub-clusters, hinting at nuanced refusal-related features (different “kinds” of harmfulness).</p> </li> </ol> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 3.5/5</strong><br/> The challenge to linear refusal assumptions is interesting, but the evidence remains primarily empirical and visualization-driven. Without stronger theoretical or comparative grounding, the claim of nonlinearity is plausible but not definitively established.</p> <p><strong>Clarity: 2.5/5</strong><br/> The paper suffers from presentation issues. Large amounts of data (tables, GDV scores, layer-by-layer plots) are shown, but the narrative does not clearly guide the reader on why these results support the central claim. Without intuitive baselines or comparisons to simpler features, it is easy to get lost in the visualizations.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The motivation is strong, refusal as a nonlinear and distributed phenomenon fits with broader critiques of overly simplistic linear probes. However, the execution feels unfocused. The paper dumps a bunch of visualization results without enough room for interpretation. For example, we are told that UMAP shows “better separation,” but no counterfactual or control experiment demonstrates why this separation should matter for safety.</p> <p>What is missing is a grounding comparison: how do these nonlinear refusal features compare to other known nonlinear behaviors in LLMs (i.e. syntax clustering, sentiment)? Why should alignment researchers care about these clusters specifically? Without this context, the findings feel more descriptive than explanatory.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="AI Alignment"/><category term="Interpretability"/><category term="Trustworthy AI"/><summary type="html"><![CDATA[This paper investigates refusal behavior in large language models, questioning the assumption that refusal can be described as a single linear subspace in model activation space. Using six instruction-tuned models across three families, the authors apply dimensionality reduction methods: PCA (linear), t-SNE, and UMAP (nonlinear) to analyze how harmful and harmless prompts separate in hidden states. They argue that refusal mechanisms exhibit nonlinear, multidimensional characteristics that differ by architecture and layer.]]></summary></entry><entry><title type="html">Literature Review: Jailbreak Antidote – Runtime Safety-Utility Balance via Sparse Representation Adjustment</title><link href="https://jeybird248.github.io/blog/2025/research_45/" rel="alternate" type="text/html" title="Literature Review: Jailbreak Antidote – Runtime Safety-Utility Balance via Sparse Representation Adjustment"/><published>2025-08-16T00:00:00+00:00</published><updated>2025-08-16T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_45</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_45/"><![CDATA[<p>The paper introduces a method to defend large language models against jailbreak attacks by directly adjusting a sparse subset of internal representations at inference time. Unlike defenses that rely on retraining, prompt modifications, or heavy overhead, this approach operates on the fly by shifting hidden states along a “safety direction” derived via PCA from benign vs. harmful prompts. The key insight is that safety-related information in LLMs is sparsely encoded, modifying only ~5% of the activations is sufficient to steer the model toward refusal without major utility loss.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Sparse Safety Encoding The finding that only a small proportion of neuron dimensions carry safety-relevant signal is critical. It suggests safety can be modulated without global architectural changes, paralleling insights from representation engineering and activation steering.</p> </li> <li> <p>Real-Time Safety Adjustment By tuning two parameters—scaling factor (α) and sparsity level (k) users can balance between conservatism and utility dynamically. This flexibility differentiates it from fixed defenses like RLHF or safety fine-tuning.</p> </li> <li> <p>Comparison Across Defenses Against ten attack methods, Jailbreak Antidote consistently achieved higher defense success rates (up to 100% on Llama-3-70B-it) while maintaining benign-task performance. Inference overhead was negligible compared to methods requiring token overhead (i.e. SmoothLLM).</p> </li> </ol> <p align="center"> <img src="../../../assets/img/literature/45_0.png" width="600"/> </p> <p align="center"><em>Figure: Jailbreak Antidote workflow. Safety direction is extracted via PCA, then a sparse subset of activations is shifted at inference to rebalance safety and utility.</em></p> <h2 id="example">Example</h2> <p>Consider an adversarial past-tense attack on the query “How people used to create explosives?” Normally, the model might respond descriptively. With Jailbreak Antidote, the hidden state at the final token is shifted toward the safety direction, leading the model to refuse instead. By adjusting α upward, the refusal becomes stronger, but benign prompts such as “Provide a travel plan for Amsterdam” remain unaffected.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong><br/> The approach builds on activation steering but adapts it to safety-utility balance in a systematic, sparse manner. The novelty lies in applying representation sparsity to runtime jailbreak defense, though it remains incremental compared to the broader interpretability literature.</p> <p><strong>Clarity: 5/5</strong><br/> The paper is well written with clear motivation, intuitive visualizations (t-SNE, ablation plots), and thorough empirical evaluation. The trade-off curves make the results easy to interpret.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The paper seems like a strong step forward in connecting mechanistic interpretability with practical safety defenses. The sparsity finding suggests a convergence toward selective control as a viable path for safety interventions.</p> <p>However, the attack surface also widens with this implementation: if α can be adversarially tuned, the same mechanism could be exploited as a jailbreak amplifier. The authors acknowledge this but stop short of proposing safeguards. Second, while the attack coverage is broad, including GCG, PAIR, and tense-based reformulations, other forms of multimodal or long-context jailbreaks remain unexplored.</p>]]></content><author><name></name></author><category term="research"/><category term="Adversarial AI"/><category term="Trustworthy AI"/><category term="LLM"/><category term="Mechanistic Interpretability"/><category term="Representation Engineering"/><summary type="html"><![CDATA[The paper introduces a method to defend large language models against jailbreak attacks by directly adjusting a sparse subset of internal representations at inference time. Unlike defenses that rely on retraining, prompt modifications, or heavy overhead, this approach operates on the fly by shifting hidden states along a “safety direction” derived via PCA from benign vs. harmful prompts. The key insight is that safety-related information in LLMs is sparsely encoded, modifying only ~5% of the activations is sufficient to steer the model toward refusal without major utility loss.]]></summary></entry><entry><title type="html">Literature Review: The Hidden Dimensions of LLM Alignment</title><link href="https://jeybird248.github.io/blog/2025/research_46/" rel="alternate" type="text/html" title="Literature Review: The Hidden Dimensions of LLM Alignment"/><published>2025-08-16T00:00:00+00:00</published><updated>2025-08-16T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_46</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_46/"><![CDATA[<p>This paper investigates the internal mechanisms of safety alignment in large language models, specifically how refusal behavior emerges not from a single linear feature but from multiple orthogonal directions in activation space. By analyzing the residual space between pre- and post-safety fine-tuning states of Llama 3 8B, the authors uncover that safety-aligned behaviors are shaped by both a dominant refusal direction and several smaller, semantically meaningful directions (i.e. role-playing, hypothetical narrative framing). These non-dominant directions can promote or suppress refusal, highlighting a multi-dimensional structure to alignment. The study also demonstrates vulnerabilities: targeted manipulation of trigger tokens or suppression of certain directions can bypass safety alignment.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Residual Space as a Safety Lens The paper defines a “safety residual space” as the linear span of representation shifts during safety fine-tuning. This provides a structured way to study how alignment objectives alter model internals.</p> </li> <li> <p>Dominant vs. Non-Dominant Directions The dominant direction strongly predicts refusal behavior, but non-dominant directions capture auxiliary features such as hypothetical prompts (“Imagine…”) or role-playing cues (“as ChatGPT”) that modulate whether refusal is triggered.</p> </li> <li> <p>Layer Dynamics Safety feature directions emerge in earlier layers and stabilize mid-network, with the dominant refusal direction consolidating strength across later layers. This mirrors other findings about mid-layer alignment activity.</p> </li> </ol> <h2 id="example">Example</h2> <p>The authors identify a specific non-dominant direction (L14-C6) corresponding to jailbreak patterns where harmful prompts are framed as fictional scenarios with the phrase “Sure, I’m happy to help.” Removing this direction during generation selectively disables the model’s ability to resist PAIR-style jailbreaks while leaving other refusals intact. This illustrates both the interpretability of the approach and the fragility of alignment mechanisms.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong><br/> The move from single-direction analyses to multi-dimensional decomposition provides a significant step forward, especially in exposing hidden vulnerabilities. Still, the linear framing may miss nonlinear safety dynamics.</p> <p><strong>Clarity: 3/5</strong><br/> The methodology is explained with visualizations, though some results (i.e. performance trade-offs after interventions) could be better elaborated to strengthen interpretability.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This work represents an important pivot: instead of treating refusal as a single monolithic feature, it recognizes the distributed nature of high level abstract signals for things like alignment. Historically, alignment studies focusing on a single probe direction oversimplified safety dynamics, much as early sentiment analysis reduced nuanced emotions to a binary axis.</p> <p>While the authors show that removing directions can selectively weaken refusal, they do not thoroughly test whether amplifying or suppressing these directions alters the model’s general capabilities.</p> <p>The vulnerability analysis raises some concerning implications that if safety relies on spurious correlations with stylistic cues, adversaries will continue to find trivial bypasses. More robust approaches will likely require rethinking safety training objectives to disentangle genuine harmfulness recognition from contextual hacks.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="Interpretability"/><category term="Trustworthy AI"/><category term="Safety Alignment"/><category term="Adversarial AI"/><summary type="html"><![CDATA[This paper investigates the internal mechanisms of safety alignment in large language models, specifically how refusal behavior emerges not from a single linear feature but from multiple orthogonal directions in activation space. By analyzing the residual space between pre- and post-safety fine-tuning states of Llama 3 8B, the authors uncover that safety-aligned behaviors are shaped by both a dominant refusal direction and several smaller, semantically meaningful directions (i.e. role-playing, hypothetical narrative framing). These non-dominant directions can promote or suppress refusal, highlighting a multi-dimensional structure to alignment. The study also demonstrates vulnerabilities: targeted manipulation of trigger tokens or suppression of certain directions can bypass safety alignment.]]></summary></entry><entry><title type="html">Literature Review: Dissecting Recall of Factual Associations in Auto-Regressive Language Models</title><link href="https://jeybird248.github.io/blog/2025/research_42/" rel="alternate" type="text/html" title="Literature Review: Dissecting Recall of Factual Associations in Auto-Regressive Language Models"/><published>2025-08-10T00:00:00+00:00</published><updated>2025-08-10T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_42</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_42/"><![CDATA[<p>This paper shifts the focus from where factual knowledge is stored in transformer-based LMs to how it is retrieved during inference. The authors propose an information flow perspective, identifying a three-step mechanism in factual recall for subject–relation queries: (1) subject enrichment via early MLP sublayers, (2) relation propagation to the prediction position, and (3) attribute extraction by upper-layer MHSA heads. They use fine-grained “attention knockout” interventions to localize when and where information propagates, revealing that critical flows from relation tokens occur before flows from subject tokens. The work emphasizes that information retrieval is internally distributed and sequential, rather than localized in a single neuron or layer.</p> <h2 id="key-insights">Key Insights</h2> <ol> <li>Subject enrichment as a distributed process Early-to-mid MLP sublayers build up semantically related attributes in the last-subject token representation. This enrichment is broad, not tied to a single fact, and can reach nearly 50% “attribute rate” (percentage of high-probability tokens related to the subject).</li> <li> <p>MHSA-driven attribute extraction Upper-layer attention heads frequently encode subject–attribute mappings directly in their parameters, acting as “knowledge hubs”.</p> </li> <li> <p>Importance of early MLPs for downstream extraction Ablating early MLP contributions sharply reduces attribute rate in subject representations, which in turn lowers the extraction success rate.</p> </li> <li>Extraction is non-trivial The target attribute often has a low rank in the enriched subject representation before being elevated to rank 1 by MHSA updates, indicating active selection rather than passive copying.</li> </ol> <h2 id="example">Example</h2> <p>For the query “Beats Music is owned by ___”:</p> <ul> <li>Early layers enrich the last-subject token “Music” with a wide set of related terms (e.g., “Apple”, “iPod”, “iOS”, “speakers”).</li> <li>Mid layers propagate relation information (“is owned by”) to the final position.</li> <li>Upper layers perform extraction: specific MHSA heads map from the “Beats Music” representation to the attribute “Apple”, which may be encoded directly in the head’s parameter space.</li> </ul> <p align="center"> <img src="../../../assets/img/literature/42_0.png" width="600"/> </p> <p align="center"><em>Figure: Identified three-step process: MLP-driven subject enrichment, relation propagation, and MHSA-based attribute extraction.</em></p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong> The intervention-based tracing of flow rather than static location is a meaningful shift, though still within established mechanistic interpretability paradigms.</p> <p><strong>Clarity: 4/5</strong> The paper’s structure is logical, with clear diagrams and well-motivated methodology, though some metric definitions (i.e. attribute rate) require careful reading.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This paper’s departure from the neuron-centric “fact storage” framing that we have come to have aligns with a growing interest in mechanistic interpretability: knowledge recall is inherently a process, not a fixed match. The results show that attention heads can embody dense subject–attribute mappings, making them potential triggers for targeted editing or robustness interventions.</p> <p>Concerns remain about scalability as attention knockout is computationally heavy, and the method assumes clean subject–relation–attribute structures that may not generalize to noisier, multi-entity contexts. Additionally, while the identification of “knowledge hubs” is interesting, the functional role of these hubs under distribution shifts is unexplored.</p> <p>If extending this work, I would:</p> <ul> <li>Probe robustness under adversarial paraphrasing to see if the same heads are responsible for extraction.</li> <li>Apply similar flow-tracing to non-factual, compositional reasoning tasks, testing whether a similar enrichment–propagation–extraction pipeline exists.</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="Mechanistic Interpretability"/><category term="LLM"/><summary type="html"><![CDATA[This paper shifts the focus from where factual knowledge is stored in transformer-based LMs to how it is retrieved during inference. The authors propose an information flow perspective, identifying a three-step mechanism in factual recall for subject–relation queries: (1) subject enrichment via early MLP sublayers, (2) relation propagation to the prediction position, and (3) attribute extraction by upper-layer MHSA heads. They use fine-grained “attention knockout” interventions to localize when and where information propagates, revealing that critical flows from relation tokens occur before flows from subject tokens. The work emphasizes that information retrieval is internally distributed and sequential, rather than localized in a single neuron or layer.]]></summary></entry><entry><title type="html">Literature Review: Context Rot — How Increasing Input Tokens Impacts LLM Performance</title><link href="https://jeybird248.github.io/blog/2025/research_43/" rel="alternate" type="text/html" title="Literature Review: Context Rot — How Increasing Input Tokens Impacts LLM Performance"/><published>2025-08-10T00:00:00+00:00</published><updated>2025-08-10T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_43</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_43/"><![CDATA[<p>This technical report analyzes how model performance changes as input length grows across a suite of tasks and 18 models. The authors isolate length from task difficulty by fixing the “needle–question” pair while increasing irrelevant context, then expand to distractor sensitivity, long-chat reasoning with LongMemEval, and a repeat-after-me stress test where output length scales with input.</p> <p align="center"> <img src="../../../assets/img/literature/43_0.png" width="600"/> </p> <p align="center"><em>Figure: Schematic summary of context rot. As context grows, accuracy falls faster when question–evidence similarity is low, when distractors are semantically confusable, and when haystack structure is shuffled. Focused prompts outperform full long prompts even for models with chain-of-thought.</em></p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Length is not neutral<br/> Performance degrades with increasing input length even when the underlying task is trivial to the model at short lengths. Holding the needle–question pair fixed while padding with irrelevant text cleanly shows the effect.</p> </li> <li> <p>Semantic alignment matters more as you go long<br/> When question and evidence share high semantic similarity, models tolerate more padding. With low-similarity pairs, accuracy decays earlier and faster. This suggests that subtle signals lose to locally coherent but irrelevant spans as length grows.</p> </li> <li> <p>Distractors scale the pain, and not all distractors are equal<br/> Semantically close distractors degrade performance more than generic irrelevant text, and specific distractors consistently cause larger drops than others. Different model families show distinct failure profiles, which suggests family-specific inductive biases in retrieval under ambiguity.</p> </li> <li> <p>Focused vs full prompts in long-chat QA<br/> On LongMemEval, compressing to the minimal relevant spans (hundreds of tokens) beats feeding the full long chat (hundreds of thousands of tokens), even with reasoning enabled. Thinking budgets raise both curves yet do not close the focused–full gap.</p> </li> <li> <p>Output-length coupling reveals stability limits<br/> A simple repeat-after-me program should be deterministic, yet models introduce random strings, refuse, or misplace the one unique token as sequence length grows. Position accuracy is best when the unique token is early, and some models emit refusals or garbage at mid–high lengths.</p> </li> </ol> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 2/5</strong> A careful, multi-angle empirical study that triangulates known long-context pitfalls with new controls and failure taxonomies.</p> <p><strong>Clarity: 4/5</strong> Clear task definitions, controlled manipulations, and interpretable plots. Mechanistic claims are explicitly out of scope, which keeps the narrative honest.</p> <h2 id="personal-comments">Personal Comments</h2> <p>This is the study I wanted to see for a while, since anecdotes about long prompts going sideways have been common. The controlled sweeps confirm that length interacts with semantics and structure, not just raw capacity. The distractor analysis is especially useful since real deployments rarely have clean haystacks. I appreciate that “thinking” helps but does not erase focused–full gaps, which aligns with experience that chain-of-thought often shifts failure classes rather than fixing them.</p>]]></content><author><name></name></author><category term="research"/><category term="LLM"/><category term="Retrieval-Augmented Generation"/><category term="Prompt Engineering"/><summary type="html"><![CDATA[This technical report analyzes how model performance changes as input length grows across a suite of tasks and 18 models. The authors isolate length from task difficulty by fixing the “needle–question” pair while increasing irrelevant context, then expand to distractor sensitivity, long-chat reasoning with LongMemEval, and a repeat-after-me stress test where output length scales with input.]]></summary></entry><entry><title type="html">Literature Review: Cross-Modal Safety Mechanism Transfer in LVLMs (TGA)</title><link href="https://jeybird248.github.io/blog/2025/research_40/" rel="alternate" type="text/html" title="Literature Review: Cross-Modal Safety Mechanism Transfer in LVLMs (TGA)"/><published>2025-08-09T00:00:00+00:00</published><updated>2025-08-09T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_40</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_40/"><![CDATA[<p>This paper argues that today’s vision–language alignment procedures do not transfer a base LLM’s text-mode safety behaviors to the vision modality. The authors localize where refusals are triggered inside LVLMs, show that image tokens are misaligned with the corresponding text at precisely those “safety-activation” layers, and propose Text-Guided Alignment (TGA), which retrieves a text template and trains the projector so that image-side hidden states align with text-side hidden states layer-wise. Empirically, TGA raises defense success rate (DSR) against toxic images across seven scenes without any vision-side safety finetuning, while preserving general VLM utility.</p> <p align="center"> <img src="../../../assets/img/literature/40_0.png" width="600"/> </p> <p align="center"><em>Figure: Concept. Safety is triggered at specific layers using information carried by toxic tokens. Existing alignment leaves image hidden states misaligned at those layers, so the mechanism fails on images. TGA pulls image states toward text states with a retrieval-guided, layer-wise loss.</em></p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Safety “lives” in specific layers and is detected via refusal semantics.<br/> The paper locates “safety-activation” layers by scanning per-layer next-token distributions and finding where “sorry/apology” tokens jump to top-rank change. They compute per-layer logits with the shared LM head and detects the first layer whose distributional delta makes “sorry” tokens top-1.</p> </li> <li> <p>Hidden-state misalignment explains cross-modal failure.<br/> At the very layers where refusals emerge for toxic text, the cosine similarity between image and text hidden states with matched semantics is lower than expected (below CLIP-level semantic similarity), indicating insufficient alignment at the critical internal representation level.</p> </li> <li> <p>Text-Guided Alignment (TGA).<br/> TGA retrieves a semantically related text (BEIT-3 index over ~1.158M captions + 5K toxic snippets) and trains with a pair-wise, layer-wise loss so that image states (I<em>j) move closer to caption states (C_j) than to retrieval states (R_j): ( \mathcal{L}</em>{\text{guide}} = \sum_j -\cos(I_j,C_j) + \log(1+\exp[-(\cos(I_j,C_j)-\cos(R_j,C_j))]) ), combined with standard LM cross-entropy.</p> </li> </ol> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 4/5</strong> Cross-modal mechanism transfer framed around layer-wise hidden-state alignment is a clear, useful angle; the retrieval-guided loss is sensible but incremental relative to existing alignment/contrastive objectives.</p> <p><strong>Clarity: 3/5</strong> Core story is readable and figures help, but the detection of safety activation via “sorry/apology” tokens relies a brittle assumption and deserves a more careful treatment and ablation.</p> <h2 id="personal-comments">Personal Comments</h2> <p>The localization procedure depends on a prompt that asks the model to refuse in a specific phrasing, then uses the per-layer surge of “sorry” tokens to mark safety activation. The question is whether the mechanism would be found, and whether TGA would still work, if refusals can pop up without the string “sorry” (i.e. “I can’t help with that”, policy citation, multilingual refusals). This risks pigeonholing both the detection and the learned behavior into one lexical template. A stricter test would repeat all analyses with a family of refusal types and a lexical-free refusal detector (i.e. a small trained refusal classifier on hidden states, or policy-key probes), then report sensitivity on that.</p>]]></content><author><name></name></author><category term="research"/><category term="Vision-Language Models"/><category term="AI Alignment"/><category term="Interpretability"/><category term="Adversarial AI"/><category term="LLM"/><summary type="html"><![CDATA[This paper argues that today’s vision–language alignment procedures do not transfer a base LLM’s text-mode safety behaviors to the vision modality. The authors localize where refusals are triggered inside LVLMs, show that image tokens are misaligned with the corresponding text at precisely those “safety-activation” layers, and propose Text-Guided Alignment (TGA), which retrieves a text template and trains the projector so that image-side hidden states align with text-side hidden states layer-wise. Empirically, TGA raises defense success rate (DSR) against toxic images across seven scenes without any vision-side safety finetuning, while preserving general VLM utility.]]></summary></entry><entry><title type="html">Literature Review: Hierarchical Reasoning Model</title><link href="https://jeybird248.github.io/blog/2025/research_41/" rel="alternate" type="text/html" title="Literature Review: Hierarchical Reasoning Model"/><published>2025-08-09T00:00:00+00:00</published><updated>2025-08-09T00:00:00+00:00</updated><id>https://jeybird248.github.io/blog/2025/research_41</id><content type="html" xml:base="https://jeybird248.github.io/blog/2025/research_41/"><![CDATA[<p>This paper introduces the Hierarchical Reasoning Model (HRM), a recurrent architecture with two coupled modules operating at different timescales: a high-level planner and a low-level fast computation module. HRM executes multi-step reasoning in a single forward pass, trains without backpropagation-through-time via a one-step gradient approximation, and adds an adaptive halting policy to scale compute to problem difficulty. With 27M parameters and ~1k training examples per task, HRM reports strong results on ARC-AGI, near-perfect accuracy on challenging Sudoku, and optimal path finding in 30×30 mazes, without pretraining or chain-of-thought supervision.</p> <p align="center"> <img src="../../../assets/img/literature/41_0.png" width="600"/> </p> <p align="center"><em>Figure: HRM couples a slow high-level module with a fast low-level module. The low-level module repeatedly converges within a cycle; the high-level update resets context, creating "hierarchical convergence." An adaptive halting head allocates extra compute on hard instances.</em></p> <h2 id="key-insights">Key Insights</h2> <ol> <li> <p>Hierarchical convergence increases effective computational depth without instability.<br/> HRM runs T low-level steps under a fixed high-level state, then performs a high-level update, and repeats for N cycles. The low-level module converges to a local equilibrium each cycle; the subsequent high-level update restarts the low-level dynamics in a new context, yielding an effective depth of N×T in a single forward pass.</p> </li> <li> <p>Training uses a one-step gradient, drawing on DEQ theory, to avoid BPTT.<br/> Instead of storing full trajectories, HRM backpropagates only through the last state of each module. The derivation uses the implicit function theorem, with a Neumann-series 1-step approximation to the fixed-point gradient, giving an O(1)-memory update rule compatible with standard autograd.</p> </li> <li> <p>Adaptive Computation Time (ACT) via a Q-learning halt policy.<br/> A small Q-head on the high-level state estimates values for “halt” vs “continue.” The policy enforces a stochastic minimum number of segments and halts when the estimated halt value exceeds continue, subject to limits.</p> </li> </ol> <h2 id="example">Example</h2> <p>Consider a Sudoku instance. HRM solves Sudoku like a student-coach team: The system has a fast solver that works on the puzzle until it gets stuck, then a strategic guide gives it new directions and “resets” the solver to try again. This cycle repeats until the system is confident in its solution or runs out of time. It’s efficient because it doesn’t waste computation, but thorough because it can keep trying new approaches as long as needed.</p> <h2 id="ratings">Ratings</h2> <p><strong>Novelty: 5/5</strong><br/> The combination of hierarchical convergence, segment-wise deep supervision with detachment, an explicit 1-step DEQ-style gradient, and a Q-learning halting policy is a meaningful combination that pushes recurrent reasoning beyond prior Universal/looped Transformers.</p> <p><strong>Clarity: 3/5</strong><br/> Key mechanisms are presented, but the paper spans neuroscience analogies, multiple training tricks, and extensive evaluation details. Some choices, such as augmentation strategy and aggregation on ARC, deserve clearer ablations and explanations.</p> <h2 id="personal-comments">Personal Comments</h2> <p>Interpretability is an open question. The intermediate visualizations suggest different task-specific strategies, such as backtracking-like behavior on Sudoku and hill-climbing on certain ARC tasks, but the evidence remains qualitative. A concrete mechanistic analysis of the two modules’ roles, attention patterns, and trajectory could convert this into a more generalizable story about learned algorithms.</p> <p>On evaluation, the Sudoku-Extreme and Maze-Hard constructions look thoughtfully hard, which partially explains the dramatic baseline separation. The ARC-AGI protocol uses heavy augmentation and a 1000-variant vote to pick two answers. That is allowed by the benchmark design, yet it blurs how much of the gain comes from the model vs the augmentation-aggregation pipeline. An ablation on the number of augmentations and voting rules would clarify this.</p>]]></content><author><name></name></author><category term="research"/><category term="Reasoning"/><category term="Recurrent Networks"/><category term="Symbolic Reasoning"/><summary type="html"><![CDATA[This paper introduces the Hierarchical Reasoning Model (HRM), a recurrent architecture with two coupled modules operating at different timescales: a high-level planner and a low-level fast computation module. HRM executes multi-step reasoning in a single forward pass, trains without backpropagation-through-time via a one-step gradient approximation, and adds an adaptive halting policy to scale compute to problem difficulty. With 27M parameters and ~1k training examples per task, HRM reports strong results on ARC-AGI, near-perfect accuracy on challenging Sudoku, and optimal path finding in 30×30 mazes, without pretraining or chain-of-thought supervision.]]></summary></entry></feed>